{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Exercises for COMP 6321 (Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you'll translate mathematics from lecture into practical Numpy code. Specifically, you'll implement _linear least squares regression_ and _logistic regression_ \"from scratch\" and compare the results of your own implementations to those of *scikit-learn*, a popular machine learning package.\n",
    "\n",
    "<span style=\"color:red\">Warning:</span> Many of the code cells in this notebook use the same simple variable names like `X` or `y` but assign them to be different data. If you run cells out of order, you may get unexpected results. If you switch between sections, it's best to re-run the relevant cells.\n",
    "\n",
    "**Run the code cell below** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab2 requires a good understanding of Numpy and Matplotlib. Please complete Lab1 before attempting Lab2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "## 1. Gradient-based optimization\n",
    "\n",
    "Exercises 1.1&ndash;1.3 are to help people who are unfamiliar with mathematical optimization or with implementing optimization in Numpy. The goal is to translate math into code on the simplest example possible.\n",
    "\n",
    "<span style=\"color:red\">*Note.*</span> We depict mathematical vectors as column-vectors but, in machine learning, it is often more convenient to stack $N$ vectors as if each vector were a row-vector, resulting in an $N \\times D$ matrix rather than a $D \\times N$ matrix. Similarly, the choice between whether to represent a Numpy vector as 1-dimensional ($\\mathbb{R}^D$) or as an explicit row-vector ($\\mathbb{R}^{1 \\times D}$) or column-vector ($\\mathbb{R}^{D \\times 1}$) can be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.1 &mdash; Compute a simple gradient with Numpy\n",
    "\n",
    "Consider the function\n",
    "$$\n",
    "f(\\mathbf{x}) = \\frac{1}{2}x_1^2 + (x_2-1)^2\n",
    "$$\n",
    "\n",
    "The gradient of this function is\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(\\mathbf{x}) \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}(\\mathbf{x}) \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "2(x_2-1) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Write two functions**, one that implements $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ and one that implements $\\nabla f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$. However, rather than assuming argument $\\mathbf{x} \\in \\mathbb{R}^2$, you should assume the argument is actually an $N \\times 2$ matrix where each row represents a distinct value of $\\mathbf{x}$ to be evaluated. This is because the functions will be used by 'vectorized' code. Your code should likewise be vectorized, without using Python loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    Given an ndarray 'x' with shape (N,2), returns an ndarray with shape (N,)\n",
    "    where the value at index i is function f(.) applied to row x[i].\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 1-3 lines.\n",
    "    x1, x2 = x.T\n",
    "    return 0.5*x1**2 + (x2-1)**2\n",
    "    \n",
    "def f_grad(x):\n",
    "    \"\"\"\n",
    "    Given an ndarray 'x' with shape (N,2), returns an ndarray with shape (N,2)\n",
    "    where the two values in row i are the gradient of f(.) applied to row x[i].\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 2-5 lines.\n",
    "    x1, x2 = x.T\n",
    "    grad = np.empty_like(x)\n",
    "    grad[:,0] = x1\n",
    "    grad[:,1] = 2*(x2 - 1)\n",
    "    return grad.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[3.5, 2.5], [-2.0, 0.5], [5.0, -3.0]])\n",
    "y = f(x)\n",
    "assert isinstance(y, np.ndarray), \"Expected f(x) to return an ndarray\"\n",
    "assert y.shape == (len(x),), \"Expected f(x) to return an ndarray of shape (N,)\"\n",
    "assert np.array_equal(y, [8.375, 2.25, 28.5]), \"Wrong values returned by f(x):\\n%s\" % y\n",
    "g = f_grad(x)\n",
    "assert isinstance(g, np.ndarray), \"Expected grad_f(x) to return an ndarray\"\n",
    "assert g.shape == x.shape, \"Expected grad_f(x) to return an ndarray of shape (N,2) when given \"\n",
    "assert np.array_equal(g, [[3.5,  3], [-2, -1], [5, -8]]), \"Wrong values returned by grad_f(x):\\n%s\" % g\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.2 &mdash; Plotting $f(\\mathbf{x})$ and $\\nabla f(\\mathbf{x})$ with vectorized code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plot the values of $f(\\mathbf{x})$ from Exercise 1.1 using $\\mathbf{x}=(x_1, x_2)$ over the square interval $[-3, 3]$, you can see the \"shape\" of the function. For example, if we use a grid size to $10 \\times 10$ we get a coarse version of the plot:\n",
    "![image](img/fig-exercise12-before-low.png)\n",
    "\n",
    "If we use a grid size of $100 \\times 100$ we get a smooth version:\n",
    "![image](img/fig-exercise12-before.png)\n",
    "We can see that the function has a global minimum at $\\mathbf{x}=(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating this kind of plot requires evaluating $f(\\mathbf{x})$ at every point in a 2D grid interval. \n",
    "The above plot was generated by a single call to Matplotlib's **[imshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.imshow.html)** function. The argument to _imshow_ was a $100 \\times 100$ matrix where entry $[i,j]$ was assigned value $f(\\mathbf{x})$ for a specific $\\mathbf{x}$ on the grid. The _imshow_ function automatically mapped different values of $f(\\mathbf{x})$ to different colours on a scale.\n",
    "\n",
    "\n",
    "In Python, efficiently evaluating $f(\\mathbf{x})$ at 10000 distinct values of $\\mathbf{x}$ requires vectorized code, so that $f$ can be called once on a big $1000 \\times 2$ array rather than being called 10000 times on small arrays of size $2$. An initial plotting function is given in the code cell below. Run it to reproduce the above plot.\n",
    "\n",
    "**Write additional lines of plotting code** to enhance the plot in two ways:\n",
    "1. Overlay the contours of $f(\\mathbf{x})$ using Matplotlib's **[contour](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.contour.html)** function.\n",
    "2. Overlay the gradient field $\\nabla f(\\mathbf{x})$ using Matplotlib's **[quiver](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.quiver.html)** function.\n",
    "\n",
    "Your final plot should look something like this:\n",
    "![image](img/fig-exercise12-solution.png)\n",
    "\n",
    "To generate the gradient field in a vectorized manner, apply the *eval_func_on_grid* function directly to your *f_grad* implementation from Exercise 1.1. For a vector field it's best to use a smaller *gridsize*, like $10 \\times 10$. If you do not understand how the vectorized code works, investigate by adding lines to print the shapes of arrays, like `print(x1.shape)`, and if that fails try asking a TA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_func_on_grid(func, gridsize):                # (You should be able to understand the code in this function)\n",
    "    steps = np.linspace(-3, 3, gridsize)              # Make list of distinct grid values ranging from -3 to 3\n",
    "    x1, x2 = np.meshgrid(steps, steps)                # Build a grid (x1[i,j], x2[i,j]) of distinct x values for each (i, j)\n",
    "    x = np.column_stack([x1.ravel(), x2.ravel()])     # Stack each pair of x values into a new (gridsize*gridsize, 2) array\n",
    "    y = func(x)                                       # Apply 'func', expecting a (gridsize*gridsize, :) array of outputs\n",
    "    y = y.reshape(gridsize, gridsize, -1).squeeze()   # Reshape y from (gridsize*gridsize, :) to (gridsize, gridsize, :)\n",
    "    return x1, x2, y\n",
    "\n",
    "def plot_exercise12():\n",
    "    # Evaluate f(x) on a dense 100x100 grid and plot it\n",
    "    x1, x2, y = eval_func_on_grid(f, 100)\n",
    "    \n",
    "    # Plot the (100,100)\n",
    "    plt.imshow(y, origin='lower', extent=(x1.min(), x1.max(), x2.min(), x2.max()))\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Configure the plot\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Plot of $f(\\\\mathbf{x})$')\n",
    "    plt.gca().set_aspect('equal')  # Force the x and y axes to scale\n",
    "\n",
    "    # YOUR ADDITIONAL PLOTTING CODE HERE. Aim for 3-4 lines.\n",
    "    plt.contour(x1, x2, y, colors='white', linestyles=':')\n",
    "    x1, x2, grad = eval_func_on_grid(f_grad, 10)\n",
    "    plt.quiver(x1, x2, grad[:,:,0], grad[:,:,1], color='white')\n",
    "    \n",
    "plot_exercise12()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.3 &mdash; Implement gradient descent on a simple function\n",
    "\n",
    "Gradient descent is an iterative algorithm that repeatedly takes steps in the direction opposite the gradient:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_\\text{new} = \\mathbf{x}_\\text{old} - \\alpha \\nabla f(\\mathbf{x}_\\text{old})\n",
    "$$\n",
    "\n",
    "The step size is scaled by the *learning rate*, which is chosen to be some constant $\\alpha \\gt 0$.\n",
    "\n",
    "**Write a function** that runs several steps of gradient descent on the function $f$ from Exercise 1.1. Use the *f_grad* function that you wrote for Exercise 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_on_f(x_init, learn_rate, num_steps):\n",
    "    \"\"\"\n",
    "    Runs num_steps of gradient descent from point x_init using\n",
    "    the given learning rate, and returns the new x coordinate.\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 4-5 lines.\n",
    "    x = x_init\n",
    "    for i in range(num_steps):\n",
    "        x = x - learn_rate*f_grad(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gradient_descent_on_f(np.array([[0, 1]]), 100.0, 1)\n",
    "assert isinstance(x, np.ndarray), \"Expected ndarray\"\n",
    "assert np.array_equal(x, [[0, 1]]), \"Gradient descent shouldn't move away from optimal value!\"\n",
    "x = gradient_descent_on_f(np.array([[1, -2]]), 0.25, 1)\n",
    "assert np.array_equal(x, [[0.75, -0.5]]), \"The first gradient step seems to be wrong!\"\n",
    "x = gradient_descent_on_f(x, 0.1, 3)\n",
    "assert np.allclose(x, [[0.54675, 0.232]]), \"The gradient seems to be wrong after a few steps!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the path of gradient descent** by running the code cell below. You should see a path of little red 'x' marks that converge near $\\mathbf{x}^*=(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_exercise12()\n",
    "learn_rate = 0.05\n",
    "x_init = np.array([[-2.0, -1.0]])                                # Start from initial point (-2, -1)\n",
    "for num_steps in range(0, 100, 5):                               # Repeatedly run gradient descent from the initial point,\n",
    "    x = gradient_descent_on_f(x_init, learn_rate, num_steps)     # eventually running it for 100 steps.\n",
    "    plt.plot(*x[0], 'xr');                                       # Add a little 'x' to the plot to show how far it got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "## 2. Linear least squares regression\n",
    "\n",
    "Exercises 2.1&ndash;2.5 ask you to implement linear least squares regression, and to compare your results to applying the scikit-learn **[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.1 &mdash; Vectorized code for generating predictions from a basic linear model\n",
    "\n",
    "Recall from Lecture 1 that a basic linear model has the form:\n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}, \\mathbf{w}) = \\mathbf{x}^T \\mathbf{w}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x} &= \\begin{bmatrix} 1 & x_1 & \\ldots & x_D \\end{bmatrix}^T\\\\\n",
    "\\mathbf{w} &= \\begin{bmatrix} w_0 & w_1 & \\ldots & w_D \\end{bmatrix}^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If both $\\mathbf{x}$ and $\\mathbf{w}$ are column vectors, the following Python function would evaluate the linear model $\\hat{y}(\\mathbf{x}, \\mathbf{w})$ correctly:\n",
    "```python\n",
    "def linear_model_predict(x, w):\n",
    "    \"\"\"Returns a prediction from linear model y(x, w) at point x using parameters w.\"\"\"\n",
    "    return x.T @ w   # Return the inner product (dot product) of vectors x and w\n",
    "```\n",
    "However, we want a version of *linear_model_predict* that vectorizes across many $\\mathbf{x}$ simultaneously. Specifically, given a matrix of inputs:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\\mathbf{x}_1^T\\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_N^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we want *linear_model_predict* to compute a vector of outputs:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\begin{bmatrix}\n",
    "\\mathbf{x}_1^T \\mathbf{w}\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{x}_N^T \\mathbf{w}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "However, if we substitute $x$ with $X$ we can no longer use expression `X.T @ w`; the matrix $X^T \\in \\mathbb{R}^{(D+1) \\times N}$ isn't even the right shape to be on the left-hand side of the product. Writing vectorized code is full of annoying little problems like this.\n",
    "\n",
    "**Write a function** that evaluates the linear model in vectorized fashion. Specifically, when given a matrix $X \\in \\mathbb{R}^{N \\times (D+1)}$ as an argument, you should figure out what mathematical expression would result in the $\\hat{\\mathbf{y}}\\in\\mathbb{R}^N$ vector shown above. Hint: the solution is only a small change from `X.T @ w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_predict(X, w):\n",
    "    \"\"\"\n",
    "    Returns predictions from linear model y(x, w) at each point X[i,:] using parameters w.\n",
    "    Given X with shape (N,D+1), w must have shape (D+1,) and the result will have shape (N,).\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 1 line.\n",
    "    return X @ w      # This works for w of 1-dimensional shape (D+1,) or of column-vector shape (D+1,1)\n",
    "    #return w @ X.T  # This also works, but will fail if w is a column vector of shape (D+1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([2, 0.5])                            # Parameters corresponding to the 1D line y = 2 + 0.5*x1\n",
    "X = np.array([[1., -3.], [1.,  3.], [1.,  5.]])   # Evaluate at x1 = -3, 2, 5\n",
    "y = linear_model_predict(X, w)                    # Predict y for all X using w\n",
    "assert isinstance(y, np.ndarray), \"Expected an ndarray!\"\n",
    "assert np.array_equal(y, [0.5, 3.5, 4.5]), \"Wrong predictions!\\n%s\" % y\n",
    "try:\n",
    "    y = linear_model_predict(X, w.reshape(-1, 1))\n",
    "except ValueError:\n",
    "    raise AssertionError(\"Your answer works when 'w' is 1-dimensional, but not when it is a column vector. Try again.\")\n",
    "w = np.array([1, 0.5, 0.25])                      # Parameters corresponding to the 2D plane y = 1 + 0.5*x1 + 0.25*x2\n",
    "X = np.array([[1., -3., 1.], [1.,  3., 0.], [1.,  5., -2.]])   # Evaluate at different (x1, x2) points\n",
    "y = linear_model_predict(X, w)                    # Predict y for all X using w\n",
    "assert np.array_equal(y, [-0.25, 2.5, 3.0]), \"Wrong predictions for 2-dimensional feature space!\\n%s\" % y\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([2, 0.5])           # Parameters corresponding to the 1D line y = 2 + 0.5*x1\n",
    "x0 = np.ones(20)                 # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 20)      # A column of x values ranging from [-5, 5]\n",
    "X = np.column_stack([x0, x1])    # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y = linear_model_predict(X, w)   # Evaluate all x values\n",
    "plt.scatter(x1, y, 10, 'r')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for linear model $y=2 + \\\\frac{1}{2}x_1$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.2 &mdash; Linear least squares regression by gradient descent\n",
    "\n",
    "Here you'll implement a 'learning' algorithm for linear least squares regression. Recall from Lecture 1 that the least squares training objective is:\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^N (y - \\hat{y}(\\mathbf{x}_i, \\mathbf{w}))^2\n",
    "$$\n",
    "\n",
    "The gradient for the above training objective is on the slide titled \"Linear least squares *learning*\" from Lecture 1. You'll need it.\n",
    "\n",
    "**Write a function** to implement linear least squares regression by gradient descent. Use the `@` operator (matrix multiplication) in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_by_gradient_descent(X, y, w_init, learn_rate=0.05, num_steps=500):\n",
    "    \"\"\"\n",
    "    Fits a linear model by gradient descent.\n",
    "    \n",
    "    If the feature matrix X has shape (N,D) the targets y should have shape (N,)\n",
    "    and the initial parameters w_init should have shape (D,).\n",
    "    \n",
    "    Returns a new parameter vector w that minimizes the squared error to the targets.\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 4-5 lines.\n",
    "    w = w_init\n",
    "    for i in range(num_steps):\n",
    "        grad = (X.T @ X) @ w - X.T @ y\n",
    "        w = w - learn_rate*grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0.0], [1, 1.0], [1, 2.0]])\n",
    "y = np.array([4.0, 3.0, 2.0])\n",
    "w = linear_regression_by_gradient_descent(X, y, np.array([0.0, 0.0]))\n",
    "assert isinstance(w, np.ndarray), \"Expected ndarray!\"\n",
    "assert w.shape == (2,), \"Wrong shape for final parameters!\\n%s\" % w\n",
    "assert np.allclose(w, [4, -1]), \"Wrong values for final parameters!\\n%s\" % w\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.3 &mdash; Linear least squares regression by direct solution\n",
    "\n",
    "As discussed in class, the optimal parameters $\\mathbf{w}^*$ for linear least squares regression can be solved *directly*, rather than iteratively.\n",
    "\n",
    "**Write a function** to solve linear least squares regression directly. Use Numpy's matrix inverse function **[np.linalg.inv](https://numpy.org/devdocs/reference/generated/numpy.linalg.inv.html)** in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_by_direct_solve(X, y):\n",
    "    \"\"\"Fits a linear model by directly solving for the optimal parameter vector w.\"\"\"\n",
    "    # Your code here. Aim for 1-2 lines.\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = linear_regression_by_direct_solve(X, y)\n",
    "assert isinstance(w, np.ndarray), \"Expected ndarray!\"\n",
    "assert w.shape == (2,), \"Wrong shape for final parameters!\\n%s\" % w\n",
    "assert np.allclose(w, [4, -1]), \"Wrong values for final parameters!\\n%s\" % w\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.4 &mdash; Run linear least squares regression and plot the result\n",
    "\n",
    "For this exercise you'll need to define Numpy arrays that correspond to the following training data:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -2.2\\\\\n",
    "1 & -0.3\\\\\n",
    "1 &  1.5\\\\\n",
    "1 &  4.8\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "-1.2 \\\\\n",
    "1.5\\\\\n",
    "4.2\\\\\n",
    "5.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Write code** to create the following plot:\n",
    "![image](img/fig-linear-regression-1d-train-and-test.png)\n",
    "\n",
    "Your code should follow this sequence of steps:\n",
    "1. Make ndarrays $X$ and $\\mathbf{y}$ that contain the above training set.\n",
    "2. Plot the training set in blue. Use the $x$ coordinates from the second column of $X$, ignoring the first column.\n",
    "3. Run linear least squares regression on $(X, \\mathbf{y})$ to get fitted parameters $\\mathbf{w}$; use your *linear_least_squares_by_direct_solve* function.\n",
    "4. Define a \"test set\" of 20 equally-spaced values of $x$ in range $[-5, 5]$. You will need to build a new matrix $X_\\text{test}$ with $1$ in the first column and the 20 distinct $x$ values in the second column. See how this is done in the last code cell of Exercise 2.1.\n",
    "5. Predict 20 $y$ values corresponding to the 20 rows of $X_\\text{test}$ by applying a linear model with your fitted parameters $\\mathbf{w}$. Do this with single call to your *linear_model_predict* function. \n",
    "6. Plot the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Define the training set. Aim for 2 lines.\n",
    "X = np.array([[1, -2.2],\n",
    "              [1, -0.3],\n",
    "              [1,  1.5],\n",
    "              [1,  4.8]])\n",
    "y = np.array([-1.2, 1.5, 4.2, 5.3])\n",
    "\n",
    "# 2. Plot the training set. Aim for 1 line.\n",
    "plt.scatter(X[:,1], y, edgecolors='b', facecolors='none', label='training points')\n",
    "\n",
    "# 3. Run linear least squares regression on the training set to compute 'w'. Aim for 1 line.\n",
    "w = linear_regression_by_direct_solve(X, y)\n",
    "\n",
    "# 4. Define the test set matrix of shape (20,2). Aim for 1-3 lines.\n",
    "X_test = np.column_stack([np.ones(20), np.linspace(-5, 5, 20)])\n",
    "\n",
    "# 5. Use the linear model to make predictions on the test set. Aim for 1 line.\n",
    "y_test = linear_model_predict(X_test, w)\n",
    "\n",
    "# 6. Plot the test predictions. Aim for 1 line, plus a few lines to configure the plot (axis labels etc).\n",
    "plt.scatter(x1, y_test, 10, 'r', label='test predictions')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"linear least squares regression\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.5 &mdash; Run scikit-learn LinearRegression\n",
    "\n",
    "The scikit-learn package provides a **[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)** object to perform linear least squares regression (also known as \"ordinary\" least squares).\n",
    "\n",
    "**Write code to fit a LinearRegression model** using the same training matrix $X$ that you defined as part of Exercise 2.4. There are only two steps:\n",
    "1. Create the _LinearRegression_ object. Use the *fit_intercept=False* option when creating the *LinearRegression* object (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)), since the $X$ matrix already has a column of 1s corresponding to an intercept parameter (the 'bias' parameter).\n",
    "2. Fit the _LinearRegression_ object to the training matrix $X$ and targets $\\mathbf{y}$. Use the object's **[fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit)** method.\n",
    "\n",
    "The variable holding a reference to your _LinearRegression_ object should be called `linear_model`, so that your answer can be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2 lines.\n",
    "linear_model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'linear_model' in globals(), \"You didn't create a variable named 'linear_model'!\"\n",
    "assert isinstance(linear_model, sklearn.linear_model.LinearRegression), \"Expected a LinearRegression instance!\"\n",
    "assert hasattr(linear_model, 'coef_'), \"No model coefficients yet! You didn't fit the model to any data!\"\n",
    "assert linear_model.intercept_ == 0.0, \"You forgot to disable fitting of the intercept!\"\n",
    "assert np.allclose(linear_model.coef_, [[1.57104472, 0.92521608]]), \"The model parameters you learned seem incorrect!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several LinearRegression model predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones(20)                        # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 20)             # A column of x values ranging from [-5, 5]\n",
    "X_test = np.column_stack([x0, x1])      # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y_test = linear_model.predict(X_test)   # Evaluate all x values\n",
    "plt.scatter(x1, y_test, 10, 'r')\n",
    "plt.scatter(X[:,1], y, edgecolor='b', facecolor='none')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for LinearRegression model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compare the model's `coef_` attribute (coefficients, i.e. model parameters) to the parameter vector $\\mathbf{w}$ that your own implementation gave from Exercise 2.4 (just use `print(w)` in your previous answer to see those values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "## 3. Logistic regression\n",
    "\n",
    "Exercises 3.1&ndash;3.4 ask you to implement logistic regression, and to compare your results to applying the scikit-learn **[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 3.1 &mdash; Vectorized code for generating predictions from a logistic model \n",
    "\n",
    "Recall from Lecture 1 that the logistic model has the form:\n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}, \\mathbf{w}) = \\sigma(\\mathbf{x}^T \\mathbf{w})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}$ and $\\mathbf{w}$ are the same as for Exercise 2.1 and $\\sigma(\\cdot)$ is the logistic sigmoid function described in Lecture 1.\n",
    "\n",
    "**Write a function** that evaluates the logistic model in vectorized fashion, just like you did for Exercise 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Returns the element-wise logistic sigmoid of z.\"\"\"\n",
    "    # Your code here. Aim for 1 line.\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def logistic_model_predict(X, w):\n",
    "    \"\"\"\n",
    "    Returns predictions from logistic model y(x, w) at each point X[i,:] using parameters w.\n",
    "    Given X with shape (N,D+1), w must have shape (D+1,) and the result will have shape (N,).\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 1-2 lines.\n",
    "    return sigmoid(X @ w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sigmoid(np.array([-1., 0., 1.5]))\n",
    "assert isinstance(y, np.ndarray), \"Expected an ndarray!\"\n",
    "assert np.allclose(y, [0.26894142, 0.5, 0.81757448]), \"Values from sigmoid() appear to be wrong!\"\n",
    "w = np.array([2, 1.5])                           # Parameters corresponding to logistic model y = sigmoid(2 + 1.5*x1)\n",
    "X = np.array([[1., -2.], [1.,  0.], [1.,  2.]])  # Evaluate at x1 = -2, 0, 2\n",
    "y = logistic_model_predict(X, w)                 # Predict y for all X using w\n",
    "assert isinstance(y, np.ndarray), \"Expected an ndarray!\"\n",
    "assert np.allclose(y, [0.26894142, 0.88079708, 0.99330715]), \"Wrong returned!\\n%s\" % y\n",
    "try:\n",
    "    y = logistic_model_predict(X, w.reshape(-1, 1))\n",
    "except ValueError:\n",
    "    raise AssertionError(\"Your answer works when 'w' is 1-dimensional, but not when it is a column vector. Try again.\")\n",
    "w = np.array([1, 0.5, 0.25])                      # Parameters corresponding to the 2D plane y = 1 + 0.5*x1 + 0.25*x2\n",
    "X = np.array([[1., -3., 1.], [1.,  3., 0.], [1.,  5., -2.]])   # Evaluate at different (x1, x2) points\n",
    "y = logistic_model_predict(X, w)                  # Predict y for all X using w\n",
    "assert np.allclose(y, [0.4378235, 0.92414182, 0.95257413]), \"Wrong predictions for 2-dimensional feature space!\\n%s\" % y\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([2, 1.5])                       # Parameters corresponding to logistic model y = sigmoid(2 + 1.5*x1)\n",
    "x0 = np.ones(20)                             # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 20)                  # A column of x values ranging from [-5, 5]\n",
    "X_test = np.column_stack([x0, x1])           # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y_test = logistic_model_predict(X_test, w)   # Evaluate all x values\n",
    "plt.scatter(x1, y_test, 10, 'r')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for logistic model $y=\\sigma(2 + \\\\frac{3}{2}x_1)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 3.2 &mdash; Logistic regression by gradient descent\n",
    "\n",
    "Recall from Lecture 1 that the basic logistic regression training objective (learning objective) is:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N y_i \\ln \\sigma(\\mathbf{w}^T \\mathbf{x}_i) + (1-y_i) \\ln \\left(1-\\sigma(\\mathbf{w}^T \\mathbf{x}_i)\\right)\n",
    "$$\n",
    "\n",
    "The \"basic\" gradient for the above training objective is on a slide titled \"Maximum likelihood estimate for LR\" from Lecture 1, and reproduced here:\n",
    "\n",
    "$$\n",
    "\\nabla \\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N (\\sigma(\\mathbf{w}^T \\mathbf{x}_i) - y_i)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "**Write a function** to implement logistic regression by gradient descent. Your answer to _logistic_regression_grad_ should ideally be fully vectorized (no for-loops), but this may take a while to figure out. If you can't figure out the vectorization, it's OK &mdash; just compute the gradient however you can. Your answer to _logistic_regression_ should use your _logistic_regression_grad_ function to compute the gradient at each step.\n",
    "\n",
    "Implementing _logistic_regression_grad_ is the hardest exercise in this lab because a vectorized implementation requires using the `@` matrix multiply operator to compute all the $\\mathbf{w}^T \\mathbf{x}$ products, reshaping the vector of residuals into a column-vector to use Numpy's broadcasting feature, and then summing over a specific axis (over training cases $i=1,\\ldots,N$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_grad(X, y, w):\n",
    "    \"\"\"Returns the gradient for basic logistic regression.\"\"\"\n",
    "    # Your code here. Aim for 1-3 lines.\n",
    "    return np.sum(X * (sigmoid(X @ w) - y).reshape(-1, 1), axis=0)\n",
    "\n",
    "def logistic_regression(X, y, w_init, learn_rate=0.05, num_steps=500):\n",
    "    \"\"\"\n",
    "    Fits a logistic model by gradient descent.\n",
    "    \n",
    "    If the feature matrix X has shape (N,D) the targets y should have shape (N,)\n",
    "    and the initial parameters w_init should have shape (D,).\n",
    "    \n",
    "    Returns a new parameter vector w that minimizes the negative log likelihood of the targets.\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 4-5 lines.\n",
    "    w = w_init\n",
    "    for i in range(num_steps):\n",
    "        grad = logistic_regression_grad(X, y, w)\n",
    "        w = w - learn_rate*grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, -1.0], [1, 1.0], [1, 2.0]])\n",
    "y = np.array([0.0, 0.0, 1.0])\n",
    "grad = logistic_regression_grad(X, y, np.array([0.0, 1.0]))\n",
    "assert isinstance(grad, np.ndarray), \"Expected ndarray from logistic_regression_grad!\"\n",
    "assert grad.shape == (2,), \"Expected gradient to have shape (2,) but was %s\" % (grad.shape,)\n",
    "assert np.allclose(grad, [0.88079708, 0.22371131]), \"Wrong value for gradient!\"\n",
    "grad = logistic_regression_grad(X, y, np.array([-1.0, 1.5]))\n",
    "assert np.allclose(grad, [0.57911459, 0.30819531]), \"Wrong value for gradient!\"\n",
    "w = logistic_regression(X, y, np.array([1.0, 0.0]))\n",
    "assert isinstance(w, np.ndarray), \"Expected ndarray from logistic_regression!\"\n",
    "assert w.shape == (2,), \"Expected parameter vector w to have shape (2,) but was %s\" % (w.shape,)\n",
    "assert np.allclose(w, [-4.14100532, 2.95489589]), \"Parameters found by gradient descent seem wrong!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3 &mdash; Run logistic regression on data and plot the result\n",
    "\n",
    "For this exercise you'll need to define Numpy arrays that correspond to the following training data:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -4.1\\\\\n",
    "1 & -2.8\\\\\n",
    "1 & -0.7\\\\\n",
    "1 &  3.5\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Write code** to create the following plot:\n",
    "![image](img/fig-logistic-regression-1d-train-and-test.png)\n",
    "\n",
    "Your code should follow this sequence of steps, which are the same as for Exercise 2.4:\n",
    "1. Make ndarrays $X$ and $\\mathbf{y}$ that contain the above training set.\n",
    "2. Plot the training set in blue. Use the $x$ coordinates from the second column of $X$, ignoring the first column.\n",
    "3. Run logistic regression on $(X, \\mathbf{y})$ to get fitted parameters $\\mathbf{w}$; use your *logistic_regression* function, starting from $\\mathbf{w}_\\text{init} = \\begin{bmatrix} 0.0, 1.0 \\end{bmatrix}^T$\n",
    "4. Define a \"test set\" of 20 equally-spaced values of $x$ in range $[-5, 5]$. You will need to build a new matrix $X_\\text{test}$ with $1$ in the first column and the 20 distinct $x$ values in the second column.\n",
    "5. Predict 20 $y$ values corresponding to the 20 rows of $X_\\text{test}$ by applying a logistic model with your fitted parameters $\\mathbf{w}$. Do this with single call to your *logistic_model_predict* function. \n",
    "6. Plot the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the training set. Aim for 2 lines.\n",
    "X = np.array([[1, -4.1],\n",
    "              [1, -2.8],\n",
    "              [1, -0.7],\n",
    "              [1,  3.5]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# 2. Plot the training set. Aim for 1 line.\n",
    "plt.scatter(X[:,1], y, edgecolors='b', facecolors='none', label='training points')\n",
    "\n",
    "# 3. Run logistic regression on the training set to get 'w'. Aim for 1-2 lines.\n",
    "w_init = np.array([0.0, 1.0])\n",
    "w = logistic_regression(X, y, w_init)\n",
    "\n",
    "# 4. Define the test set matrix of shape (20,2). Aim for 1-3 lines.\n",
    "X_test = np.column_stack([np.ones(20), np.linspace(-5, 5, 20)])\n",
    "\n",
    "# 5. Use the linear model to make predictions on the test set. Aim for 1 line.\n",
    "y_test = logistic_model_predict(X_test, w)\n",
    "\n",
    "# 6. Plot the test predictions. Aim for 1 line, plus a few lines to configure the plot (axis labels etc).\n",
    "plt.scatter(x1, y_test, 10, 'r', label='test predictions')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"logistic regression\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 3.4 &mdash; Run scikit-learn LogisticRegression\n",
    "\n",
    "The scikit-learn package provides a **[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** object to perform logistic regression.\n",
    "\n",
    "**Write code to fit a LogisticRegression model** using the same training matrix $X$ that you defined as part of Exercise 3.3. There are only two steps:\n",
    "1. Create the _LogisticRegression_ object. Use the *fit_intercept=False*, *penalty='none'*, and *solver='lbfgs'* options when creating the *LogisticRegression* object (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). (L-BFGS is a more powerful gradient-based solver than mere gradient descent.)\n",
    "2. Fit the _LogisticRegression_ object to the training matrix $X$ and targets $\\mathbf{y}$. Use the object's **[fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit)** method.\n",
    "\n",
    "The variable holding a reference to your _LogisticRegression_ object should be called `model`, so that your answer can be checked.\n",
    "\n",
    "A tweet regarding the fact that scikit-learn's LogisticRegression object applies regularization (a weight penalty) \"by default\":\n",
    "![image](img/fig-logistic-regression-regularization-tweet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2 lines.\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(fit_intercept=False, penalty='none', solver='lbfgs')\n",
    "logistic_model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'logistic_model' in globals(), \"You didn't create a variable named 'logistic_model'!\"\n",
    "assert isinstance(logistic_model, sklearn.linear_model.LogisticRegression), \"Expected a LogisticRegression instance!\"\n",
    "assert hasattr(logistic_model, 'coef_'), \"No model coefficients yet! You didn't fit the model to any data!\"\n",
    "assert logistic_model.intercept_ == 0.0, \"You forgot to disable fitting of the intercept!\"\n",
    "assert np.allclose(logistic_model.coef_, [[18.5251137, 10.49283446]]), \"The parameters seem incorrect! Not L-BFGS?\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model parameters (coefficients) found by the _LogisticRegression_ are much larger than those found by your gradient descent solver. That is only because scikit-learn uses a more powerful optimization algorithm and can learn very sharp decision boundaries in fewer steps than mere gradient descent can. If you increase your *num_steps* argument your solver will find similarly large coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several LogisticRegression predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones(50)                               # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 50)                    # A column of x values ranging from [-5, 5]\n",
    "X_test = np.column_stack([x0, x1])             # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y_test = logistic_model.predict_proba(X_test)  # Evaluate all x values and get two probabilities back (class 0, class 1)\n",
    "plt.scatter(x1, y_test[:,1], 10, 'r')          # Plot probability of class 1 only\n",
    "plt.scatter(X[:,1], y, edgecolor='b', facecolor='none')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for LogisticRegression model\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
