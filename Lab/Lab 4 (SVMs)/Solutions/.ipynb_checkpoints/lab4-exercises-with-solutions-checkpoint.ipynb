{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 Exercises for COMP 6321 (Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you'll train a support vector machine (SVM) for classification. You'll do so on both synthetic and real data. You'll also see that SVMs are sensitive to the relative scale of your features.\n",
    "\n",
    "**Run the code cell below** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sklearn\n",
    "import sklearn.svm             # For SVC class\n",
    "import sklearn.preprocessing   # For scale function\n",
    "import sklearn.metrics         # for classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 1. Fitting an SVM to synthetic data\n",
    "\n",
    "Exercises 1.1&ndash;1.7 ask you to apply scikit-learn's support vector classifier (**[sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)**) to synthetic data. _SVC_ object is a binary classifier, so it is used much the same way as **[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)**. The goal of Exercise 1 is to connect mathematical concepts from lecture to the _SVC_ object's basic parameters (*C*, *kernel*, *gamma*, *degree*, *coef0*) and attributes (*support_*, *support_vectors_*, *dual_coef_*, *n_support_*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.1 &mdash; Build the 1D training data from lecture\n",
    "\n",
    "Read the documentation for the *SVC* **[fit](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit)** method, specifically arguments $X$ (the training features) and $y$ (the training targets). In lecture we called the targets $t_i$ (like the Bishop book) rather than $y_i$ (sci-kit learn) so we'll continue using $t_i$ below.\n",
    "\n",
    "You are asked to build the small training set below (from Lecture 3) where each pair $(x_i, t_i)$ is a _feature_ $x_i \\in \\mathbb{R}$ and its corresponding _class label_ $t_i \\in \\{-1, +1\\}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(2,-1), (8,+1), (10,+1)\\}\n",
    "$$\n",
    "\n",
    "This training set can be depicted as below, where red indicates negative class and blue indicates positive class:\n",
    "![image](img/toydata.png)\n",
    "\n",
    "**Write a few lines of code** to define a variable $X$ that refers to a $3 \\times 1$ matrix of features (dtype *float64*), and a variable $t$ that refers to a length-3 array of targets (dtype *int32*). The features and targets should correspond to $\\mathcal{D}$ above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-4 lines.\n",
    "X = np.array([[2.], [8.], [10.]])\n",
    "t = np.array([-1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'X' in globals(), \"No X variable!\"\n",
    "assert 't' in globals(), \"No t variable!\"\n",
    "assert isinstance(X, np.ndarray)\n",
    "assert isinstance(t, np.ndarray)\n",
    "assert X.shape == (3, 1)\n",
    "assert t.shape == (3,)\n",
    "assert X.dtype == np.float64\n",
    "assert t.dtype == np.int32\n",
    "assert np.array_equal(X.ravel()[[-1,0,-2]], [10,2,8]), \"Hmm features look wrong\"\n",
    "assert np.array_equal(t.ravel()[[-1,0,-2]], [1,-1,1]), \"Hmm targets look wrong\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.2 &mdash; Train an SVM on the 1D data and inspect the support vectors\n",
    "\n",
    "Read the first few lines of documentation for **[sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)** to learn how to at least create an _SVC_ object. You only have to worry about the _kernel_ parameter for now, not the rest. You are asked to create an _SVC_ object that uses a _linear_ kernel and fit it to training data.\n",
    "\n",
    "**Write a few line of code** to create a variable called _svm_ that refers to a new _SVC_ object. Fit the SVM to the training data from Exercise 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "svm = sklearn.svm.SVC(kernel='linear')\n",
    "svm.fit(X, t);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'svm' in globals(), \"No variable called 'svm' was found!\"\n",
    "assert isinstance(svm, sklearn.svm.SVC), \"Expected svm to be an SVC instance!\"\n",
    "assert svm.kernel == 'linear', \"Expected linear kernel!\"\n",
    "assert svm.fit_status_ == 0, \"Forgot to train the SVM!\"\n",
    "assert hasattr(svm, 'dual_coef_'), \"Forgot to train the SVM!\"\n",
    "assert np.array_equal(X[svm.support_], [[2.], [8.]]), \"Hmm the support vectors don't look right!\"\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect the SVM parameters** by running the code cell below. How do they compare to the values from Lecture 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Support vector indices:\")\n",
    "print(svm.support_)\n",
    "\n",
    "print()\n",
    "print(\"Support vectors:\")\n",
    "print(svm.support_vectors_)\n",
    "\n",
    "print()\n",
    "print(\"Dual coefficients (t_i * alpha_i) for the support vectors:\")\n",
    "print(svm.dual_coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.3 &mdash; Plot the decision function and class predictions\n",
    "\n",
    "Here you are asked to plot the SVM \"decision function\" $y(x)$ and the SVM classification $\\mathrm{sign}(y(x))$. These are provided by the _SVC_ **[decision_function](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function)** and **[predict](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict)** methods respectively. Evaluate them both across the range $x \\in[0, 12]$. Your final plot should look like this:\n",
    "![image](img/svm-prediction-toy-1.png)\n",
    "\n",
    "**Write code** to generate the plot above, using **[np.linspace](https://numpy.org/devdocs/reference/generated/numpy.linspace.html)** to create a vector of values spanning the range $[0,12]$. To highlight the support vectors, use the *support_* attribute of your _SVC_ object. Your code should be completely vectorized, with no for-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_toy_1d_data(X, t, title, support=None):  # You can use this function throughout the lab\n",
    "    \"\"\"\n",
    "    Plots 1-dimensional data X with targets t.\n",
    "    \n",
    "    If 'support' is given, it specifies the indices of data points in X that\n",
    "    are the support vectors of an SVM. Those points will be circled to highlight them.\n",
    "    \"\"\"\n",
    "    plt.scatter(X[t==-1], t[t==-1], s=50, edgecolors='r', facecolors='none', label='negative data')\n",
    "    plt.scatter(X[t==+1], t[t==+1], s=50, edgecolors='b', facecolors='none', label='positive data')\n",
    "    if support is not None:\n",
    "        plt.scatter(X[support], t[support], s=200, edgecolors='g', facecolors='none')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "# Your code here. Aim for 4-6 lines. You can call the above function too.\n",
    "xvals = np.linspace(0, 12, 500).reshape(-1, 1)\n",
    "plt.plot(xvals, svm.decision_function(xvals), 'g', label='svm.decision_function')\n",
    "plt.plot(xvals, svm.predict(xvals),           'k', label='svm.predict')\n",
    "plot_toy_1d_data(X, t, 'Predictions of 1D linear SVM', svm.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.4 &mdash; Compare SVM to Logistic Regression\n",
    "\n",
    "A 1-dimensional logistic regression classifier predicts class probabilities using the form $\\sigma(w_1 x + w_0)$. The quantity $\\hat{y}(x)=w_1 x + w_0$ used as input to the sigmoid is the classifier's \"decision function,\" and it plays the same role as the decision function of an SVM: the actual class prediction for $x$ can be written $\\mathrm{sign}(\\hat{y}(x)) \\in \\{-1, +1\\}$.\n",
    "\n",
    "Here you are asked to train a **[sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** object on the same data, and compare is decision function and predictions to that of the SVM. You should end up with the following plot:\n",
    "\n",
    "![image](img/svm-prediction-toy-2.png)\n",
    "\n",
    "\n",
    "**Write code** to train a _LogisticRegression_ object (use *solver='lbfgs'* as an argument) and to generate a new plot that includes the result of the **[decision_function](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.decision_function)** and **[predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict)** methods of _LogisticRegression_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your training code here. Aim for 1-2 lines.\n",
    "lr = sklearn.linear_model.LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X, t)\n",
    "\n",
    "# Your prediction and plotting code here. Aim for 5-7 lines.\n",
    "plt.plot(xvals, svm.decision_function(xvals), 'g', label='svm.decision_function')\n",
    "plt.plot(xvals, svm.predict(xvals),           'k', label='svm.predict')\n",
    "plt.plot(xvals, lr.decision_function(xvals), ':g', label='lr.decision_function')\n",
    "plt.plot(xvals, lr.predict(xvals),           ':k', label='lr.predict')\n",
    "plot_toy_1d_data(X, t, 'Comparison of 1D linear SVM with LR', svm.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, try setting _LogisticRegression_ regularization parameter $C=0.1$ to see how it affects the decisoin function of logistic regression. A regularization coefficient of $C=0.1$ in scikit-learn corresponds to a regularization coefficient $\\lambda=10$ that we saw for Regularized Least Squares in Lecture 1, but applied to the weights of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.4 &mdash; Build a non-separable 1D data set\n",
    "\n",
    "Update your $X$ matrix and $t$ vector to include a new $4^\\text{th}$ point $(x_4, t_4) = (11, -1)$. This will make the data *non-separable* in one dimension.\n",
    "\n",
    "**Write code** to define new _X_ and _t_ variables with the same data as Exercise 1.1 but this time with the 4th data point. Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-4 lines.\n",
    "X = np.array([[2.], [8.], [10.], [11.]])\n",
    "t = np.array([-1, 1, 1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.5 &mdash; Fit a linear SVM to the non-separable data\n",
    "\n",
    "**Write code** to fit an _SVC_ object with linear kernel to this new data and plot the decision function just as before. You should get the plot below. What changed in terms of the decision function and decision boundary? What changed in terms of the support vectors?\n",
    "\n",
    "![image](img/svm-prediction-toy-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your training code here. Aim for 1-2 lines.\n",
    "svm = sklearn.svm.SVC(kernel='linear')\n",
    "svm.fit(X, t)\n",
    "\n",
    "# Your prediction and plotting code here. Aim for 3-5 lines.\n",
    "plt.plot(xvals, svm.decision_function(xvals), 'g', label='svm.decision_function')\n",
    "plt.plot(xvals, svm.predict(xvals),           'k', label='svm.predict')\n",
    "plot_toy_1d_data(X, t, 'SVM with linear kernel on non-separable 1D data', svm.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.6 &mdash; Fit a polynomial SVM to the non-separable data\n",
    "\n",
    "**Repeat Exercise 1.5** using an _SVC_ object with a \"polynomial kernel\", which in one dimension is $k(x, x') = (x x' + c)^d$.\n",
    "\n",
    "\n",
    "See the **[sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)** documentation for how to specify the kernel and related parameters. Use polynomial degree $d=2$ and try different coefficients for the constant different $c$ such as $\\{0, 0.1, 1, 2, 3\\}$ until you get a plot similar to the one below. Note that these parameters are called _degree_ and _coef0_ on the _SVC_ object. (Scikit-learn's polynomial kernel also has a _gamma_ scaling factor; just set *gamma=1* for this exercise.)\n",
    "\n",
    "![image](img/svm-prediction-toy-4.png)\n",
    "\n",
    "Ask yourself:\n",
    "* *Would this fit be possible if we tried to fit a regular polynomial to this data, rather than an SVM?*\n",
    "* *Does the first decision threshold seem like its maximizing the margin in the original 1-dimensional feature space?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your training code here. Aim for 1-2 lines.\n",
    "svm = sklearn.svm.SVC(kernel='poly', degree=2, coef0=3., gamma=1.)\n",
    "svm.fit(X, t)\n",
    "\n",
    "# Your prediction and plotting code here. Aim for 3-5 lines.\n",
    "plt.plot(xvals, svm.decision_function(xvals), 'g', label='svm.decision_function')\n",
    "plt.plot(xvals, svm.predict(xvals),           'k', label='svm.predict')\n",
    "plot_toy_1d_data(X, t, 'SVM with quadratic kernel on non-separable 1D data', svm.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try setting** coefficient $c=0$ and degree $d=4$ (or higher) and re-run your code cell above. Notice how the training time suddenly gets noticeably longer. In real-life this can be a big problem with using SVMs. Keep in mind that this is an embarrassingly small data set, and scikit-learn relies on the LIBSVM library, one the most specialized and highly-optimized SVM solvers in the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.7 &mdash; Fit an RBF SVM (Gaussian kernel) to the non-separable data\n",
    "\n",
    "**Repeat Exercise 1.5** using an _SVC_ object with a \"radial basis function (RBF) kernel,\" which in one dimension is $k(x, x') = \\exp \\left(-\\gamma \\left| x - x' \\right|^2\\right)$ where $\\gamma$ is the 'spread' coefficient.\n",
    "\n",
    "See the **[sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)** documentation for how to specify the RBF kernel, and see the Lecture 3 notes on \"Gaussian kernel\" for description of how it is influenced by the _gamma_ ($\\gamma$) coefficient. The _degree_ and _coef0_ parameters are not used for RBF kernels.\n",
    "\n",
    "Use $\\gamma=1$ to get a plot similar to the one below.\n",
    "\n",
    "![image](img/svm-prediction-toy-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your training code here. Aim for 1-2 lines.\n",
    "svm = sklearn.svm.SVC(kernel='rbf', gamma=1.)\n",
    "svm.fit(X, t)\n",
    "\n",
    "# Your prediction and plotting code here. Aim for 3-5 lines.\n",
    "plt.plot(xvals, svm.decision_function(xvals), 'g', label='svm.decision_function')\n",
    "plt.plot(xvals, svm.predict(xvals),           'k', label='svm.predict')\n",
    "plot_toy_1d_data(X, t, 'SVM with RBF kernel on non-separable 1D data', svm.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify the spread coefficient** to be a large value like $\\gamma=10$ and re-run your code cell above. What happens to the decision function? Does anything happen to the actual decision boundary? What happens to the rightmost decision threshold when you make $\\gamma=0.1$, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 2. Loading real data and fitting an SVM to it\n",
    "\n",
    "Exercises 2.1&ndash;2.3 ask you to load a real data set, train an SVM on it, and make predictions on new test data.\n",
    "\n",
    "**Run the code cell below** to define some utility functions you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_extent(X):\n",
    "    \"\"\"\n",
    "    Given an Nx2 matrix X, returns a good range of values for plotting\n",
    "    the data, in the form (x1min, x1max, x2min, x2max).\n",
    "    \"\"\"\n",
    "    dilation = 1.2\n",
    "    x1min, x2min = X.min(axis=0)\n",
    "    x1max, x2max = X.max(axis=0)\n",
    "    x1mid = (x1max + x1min)/2\n",
    "    x2mid = (x2max + x2min)/2\n",
    "    x1min = x1mid - (x1mid - x1min)*dilation\n",
    "    x1max = x1mid + (x1max - x1mid)*dilation\n",
    "    x2min = x2mid - (x2mid - x2min)*dilation\n",
    "    x2max = x2mid + (x2max - x2mid)*dilation\n",
    "    return (x1min, x1max, x2min, x2max)\n",
    "    \n",
    "def plot_2d_decision_function(model, extent):\n",
    "    \"\"\"\n",
    "    Plots the decision function of a model as a red-blue heatmap.\n",
    "    The region evaluated, along with x and y axis limits, are determined by 'extent'.\n",
    "    \"\"\"\n",
    "    x1min, x1max ,x2min, x2max = extent    \n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 200),\n",
    "                         np.linspace(x2min, x2max, 200))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    y = model.decision_function(X).reshape(x1.shape)\n",
    "    plt.imshow(-y, extent=extent, origin='lower', vmin=-1, vmax=1, cmap='bwr', alpha=0.5)\n",
    "    plt.contour(x1, x2, y, levels=[0], colors='k')  # Decision boundary\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    plt.gca().set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.1 &mdash; Load data from a CSV file and plot it\n",
    "\n",
    "CSV files contain comma-separated data, sometimes with a header line to hint at what the numbers mean. In this exercise you'll be loading **[data_train.csv](data_train.csv)**, a file accompanying this lab.Here's a preview of its contents:\n",
    "```\n",
    "mean_texture,mean_compactness,label\n",
    "19.59,0.08,0\n",
    "17.88,0.16,1\n",
    "17.60,0.17,1\n",
    "10.91,0.05,0\n",
    "13.16,0.09,0\n",
    "...\n",
    "```\n",
    "The first two comman-separated columns are features. They encode characteristics of cell nuclei in breast cancer samples. The labels are binary: 0 for benign, 1 for malignant.\n",
    " \n",
    "**Write a few lines of code** to:\n",
    "1. load this CSV file from disk into a single array,\n",
    "2. split the columns into feature matrix $X$ and target vector $t$, and\n",
    "3. rescale the targets $t$ from $\\{0,1\\}$ to integers $\\{-1,+1\\}$.\n",
    "\n",
    "Use the **[np.loadtxt](https://numpy.org/devdocs/reference/generated/numpy.loadtxt.html)** function to load the data for you. Use the _delimiter_ parameter to tell Numpy how to separate each line (by comma) and use the _skiprows_ argument to tell Numpy to skip the header line that contains the feature names (since the header line contains text, not numbers). Use the ndarray **[astype](https://numpy.org/devdocs/reference/generated/numpy.ndarray.astype.html)** method to convert the targets from type *np.float64* to type *np.int32*, since they are integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 3 lines.\n",
    "data = np.loadtxt('data_train.csv', delimiter=',', skiprows=1)\n",
    "X = data[:,:2]\n",
    "t = data[:,2].astype(np.int32)*2 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'X' in globals(), \"No X variable!\" \n",
    "assert 't' in globals(), \"No t variable!\" \n",
    "assert isinstance(X, np.ndarray)\n",
    "assert isinstance(t, np.ndarray)\n",
    "assert X.shape == (100,2), \"X was wrong shape!\"\n",
    "assert X.dtype in (np.float32, np.float64), \"X was wrong data type!\"\n",
    "assert t.shape == (100,), \"t was wrong shape!\"\n",
    "assert t.dtype == np.int32, \"t was wrong data type!\"\n",
    "assert np.array_equal(X[0], [19.59, 0.08]), \"Wrong features in X!\"\n",
    "assert np.array_equal(X[-1], [16.03, 0.06]), \"Wrong features in X!\"\n",
    "assert np.array_equal(t[0:6], [-1,1,1,-1,-1,-1]), \"Wrong labels in t!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write plotting code** to plot your features data in two dimensions. Your plot should look like this:\n",
    "![image](img/svm-breast-data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_breast_data(X, t, title):\n",
    "    # Your code here. Aim for 2 lines, plus a few for labels/title/legend.\n",
    "    plt.scatter(X[t==-1,0], X[t==-1,1], s=30, c='r', marker='x', label='benign')\n",
    "    plt.scatter(X[t==+1,0], X[t==+1,1], s=30, c='b', marker='x', label='malignant')\n",
    "    plt.xlabel('mean_texture')\n",
    "    plt.ylabel('mean_compactness')\n",
    "    plt.title(title)\n",
    "    plt.legend();\n",
    "    \n",
    "plot_breast_data(X, t, 'breast cancer training data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.2 &mdash; Train an RBF SVM on the breast cancer data\n",
    "\n",
    "You must train an RBF SVM on the breast cancer data. Your final result should look like this:\n",
    "\n",
    "![image](img/svm-breast-rbf-1.png)\n",
    "\n",
    "If your decision function does _not_ look like the above, then take note of the relative scale of the features and consider preprocessing your data (see end of Lecture 1). Do you understand why the RBF kernel gave such terrible predictions on the 'raw' features?\n",
    "\n",
    "**Write a few lines of code** to train an _SVC_ object on the data and plot the resulting predictor. Use $\\gamma=1$ for the RBF kernel. Try to make your plot have equal aspect ratio between $x$ and $y$ axes using the line `plt.gca().set_aspect('equal')`. **Optional:** also plot the support vectors using Matplotlib's _scatter_ function, like the *plot_toy_1d_data* function did from Exercise 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your training code here. Aim for 3 lines.\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "svm = sklearn.svm.SVC(kernel='rbf', gamma=1.)\n",
    "svm.fit(X_scaled, t)\n",
    "\n",
    "# Your plotting code here. Aim for 4-5 lines. You can use plot_2d_decision_function defined earlier.\n",
    "plot_2d_decision_function(svm, get_data_extent(X_scaled))\n",
    "plot_breast_data(X_scaled, t, 'SVM with RBF kernel')\n",
    "plt.scatter(*X_scaled[svm.support_].T, s=100, edgecolor=[0,1,0], facecolor='none')\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try different spread coefficients** by setting $\\gamma=0.1$ and $\\gamma=10$. What do you observe in terms of the decision boundary? What do you observe in terms of the number of support vectors?. When finished, re-train your model with the original $\\gamma=1$ and proceed to the final exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.3 &mdash; Evaluate your SVM on held-out test data\n",
    "\n",
    "Here you must use your _SVC_ object from Exercise 2.2 to make predictions on data from **[data_test.csv](data_test.csv)**, a held-out test set for the breast cancer data.\n",
    "\n",
    "**Write a few lines of code** to load the features and labels for the test data (just like you did for the training data in Exercise 2.1). Then make predictions on the test set. To see what fraction of your SVM predictions were correct on the test set, read the documentation for the **[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)** function and print the accuracy that it returns. If your accuracy is below 80%, then maybe you likely didn't process your test features correctly (see end of Lecture 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data loading code here. Aim for 3-4 lines.\n",
    "data = np.loadtxt('data_test.csv', delimiter=',', skiprows=1)\n",
    "X_test = data[:,:2]\n",
    "t_test = data[:,2].astype(np.int32)*2-1\n",
    "\n",
    "# Your prediction and reporting code here. Aim for 2-3 lines.\n",
    "t_pred = svm.predict(scaler.transform(X_test))\n",
    "sklearn.metrics.accuracy_score(t_test, t_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
