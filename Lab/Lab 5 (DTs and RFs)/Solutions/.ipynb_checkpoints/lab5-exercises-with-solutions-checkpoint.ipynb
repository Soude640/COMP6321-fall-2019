{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 Exercises for COMP 6321 (Machine Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you'll train a decision tree classifier and a random forest classifier. You'll do so on both synthetic and real data.\n",
    "\n",
    "**Run the code cell below** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.tree        # For DecisionTreeClassifier class\n",
    "import sklearn.ensemble    # For RandomForestClassifier class\n",
    "import sklearn.datasets    # For make_circles\n",
    "import sklearn.metrics     # For accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 1. Fitting a Random Forest classifier to synthetic data\n",
    "\n",
    "Exercises 1.1&ndash;1.4 ask you to apply scikit-learn's decision tree classifier (**[sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)**) and random forest classifier (**[sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**) to synthetic data.\n",
    "\n",
    "**Run the code cell below** to define some useful functions for plotting data and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y):\n",
    "    \"\"\"Plots a toy 2D data set. Assumes values in range [-3,3] and at most 3 classes.\"\"\"\n",
    "    plt.plot(X[y==0,0], X[y==0,1], 'ro', markersize=6)\n",
    "    plt.plot(X[y==1,0], X[y==1,1], 'bs', markersize=6)\n",
    "    plt.plot(X[y==2,0], X[y==2,1], 'gx', markersize=6, markeredgewidth=2)\n",
    "    plt.xlim([-3, 3])\n",
    "    plt.ylim([-3, 3])\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.gca().set_aspect('equal')\n",
    "    \n",
    "def plot_predict(model):\n",
    "    \"\"\"\n",
    "    Plots the model's predictions over all points in range 2D [-3, 3].\n",
    "    Assumes at most 3 classes.\n",
    "    \"\"\"\n",
    "    extent = (-3, 3, -3, 3)\n",
    "    x1min, x1max ,x2min, x2max = extent\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 100), np.linspace(x2min, x2max, 100))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    y = model.predict(X).reshape(x1.shape)\n",
    "    cmap = matplotlib.colors.ListedColormap(['r', 'b', 'g'])\n",
    "    plt.imshow(y, extent=extent, origin='lower', alpha=0.4, vmin=0, vmax=2, cmap=cmap)\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    \n",
    "def plot_class_probability(model, class_index):\n",
    "    \"\"\"\n",
    "    Plots the model's class probability for the given class {0,1,2}\n",
    "    over all points in range 2D [-3, 3]. Assumes at most 3 classes.\n",
    "    \"\"\"\n",
    "    extent = (-3, 3, -3, 3)\n",
    "    x1min, x1max ,x2min, x2max = extent\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 100), np.linspace(x2min, x2max, 100))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    p = model.predict_proba(X)[:,class_index].reshape(x1.shape)\n",
    "    colors = [[1, 0, 0], [0, 0, 1], [0, 1, 0]]\n",
    "    cmap = matplotlib.colors.ListedColormap(np.linspace([1, 1, 1], colors[class_index], 50))\n",
    "    plt.imshow(p, extent=extent, origin='lower', alpha=0.4, vmin=0, vmax=1, cmap=cmap)\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    plt.gca().set_aspect('equal')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.1 &mdash; Train and inspect a small decision tree (2 points, 2 classes)\n",
    "\n",
    "Read the documentation for the *DecisionTreeClassifier*'s **[fit](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit)** method. Notice that the $y$ vector should contain integer class labels.\n",
    "You are asked to build the small 2D training set below:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "-1 & 0\\\\\n",
    " 1 & 0\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "y = \\begin{bmatrix}\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Write a few lines of code** to\n",
    "1. Define the training set above in two variables $X$ and $y$.\n",
    "2. Train a decision tree classifier on $X$ and $y$. Use argument *random_state*=0.\n",
    "3. Plot the decision tree predictions and the data (use *plot_predict* and *plot_data* from preamble).\n",
    "4. Plot the decision tree itself (use **[sklearn.tree.plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)**); pass `feature_names=['x1', 'x2']` as an argument.\n",
    "\n",
    "You should end up with a plot showing the data and the decision surface between classes 0 (red) and class 1 (blue). You should see a binary decision tree diagram depicting a tree of height 1 that splits the feature space using the first variable (`x1`) at threshold 0.\n",
    "\n",
    "*Tip 1:* If your code cell only outputs one figure, use `plt.figure()` to tell Matplotlib that you want the subsequent plotting commands to affect a new, separate figure.\n",
    "\n",
    "*Tip 2:* If the last line of your code cell returns a value, it will be printed as the `Out` of the cell before the plots are shown. Some times you don't care about this 'final' value, for example if it is a string or some object you don't need printed. If you want to suppress the cell's `Out` value, add a semicolon (`;`) to the end of the last line of code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 7-9 lines.\n",
    "X = np.array([[-1, 0],\n",
    "              [-1/3, 0],\n",
    "              [1/3, 0],\n",
    "              [ 1, 0]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "tree = sklearn.tree.DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X, y)\n",
    "\n",
    "plot_predict(tree)\n",
    "plot_data(X, y)\n",
    "\n",
    "plt.figure()\n",
    "sklearn.tree.plot_tree(tree, feature_names=['x1', 'x2']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have things working, **add two data points** to your training set: \n",
    "* $\\mathbf{x}_3 = (-\\frac{1}{3}, 0)$ with class label $y_3=0$ (red), and\n",
    "* $\\mathbf{x}_4 = (\\frac{1}{3}, 0)$ with class label $y_4 = 1$ (blue).\n",
    "\n",
    "**Re-run your code cell above** and make sure you understand how the splits and thresholds you see in the tree correspond to the decision region shown.\n",
    "\n",
    "*Note:* If a decision tree node is shown as having *value*=[1,2], it means that node's region (before splitting) contains exactly one training point from class 0 and two training points from class 1. The root note thus 'contains' the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.2 &mdash; Train and inspect a small decision tree (3 points, 3 classes)\n",
    "\n",
    "**Repeat Exercise 1.1** but with the following changes:\n",
    "1. Add an extra training point $\\mathbf{x}_3 = (0, -2)$ with class label $y_3=2$.\n",
    "2. Print the **[feature_importances_](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)** attribute of your trained decision tree. Intuitively, the feature importance is 0.0 if a feature is not used at all, or 1.0 if it is the only feature needed to make decisions.\n",
    "\n",
    "This time you should see a binary decision tree of height 2, where the first split is done by thresholding the second feature (`X[1]`) and the second split is done by thresholding the first feature (`X[0]`). If a node has *value*=[1,1,0], it means that node's region (say, the red+blue region) contains exactly one training point from class 0, one training point from class 1, and zero training points from class 2.\n",
    "\n",
    "3. Try incrementing *random_state* from 0 up to 9. How many distinct decision trees did you observe? Do the 'feature importances' make intuitive sense, given the trees that you observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 9-11 lines.\n",
    "X = np.array([[-1,  0],\n",
    "              [ 1,  0],\n",
    "              [ 0, -2]])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "tree = sklearn.tree.DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X, y)\n",
    "print(\"feature importances:\", tree.feature_importances_)\n",
    "\n",
    "plot_predict(tree)\n",
    "plot_data(X, y)\n",
    "\n",
    "plt.figure()\n",
    "sklearn.tree.plot_tree(tree);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.3 &mdash; Train and inspect a small random forest (3 points, 3  classes)\n",
    "\n",
    "The goal of this exercise is to show you how a random forest is a collection of decision trees.\n",
    "\n",
    "**Repeat Exercise 1.2** but this time train using a random forest classifier (**[sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**) on the same 3-point data set. Use *random_state*=0 and *n_estimators*=3. You should see a decision region that still has axis-aligned boundaries, but different from Exercise 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 9-11 lines. (OK to paste your data set building code from Exercise 1.2.)\n",
    "X = np.array([[-1,  0],\n",
    "              [ 1,  0],\n",
    "              [ 0, -2]])\n",
    "y = np.array([0, 1, 2])\n",
    "\n",
    "forest = sklearn.ensemble.RandomForestClassifier(random_state=0, n_estimators=3)\n",
    "forest.fit(X, y)\n",
    "print(\"feature importances:\", forest.feature_importances_)\n",
    "\n",
    "plot_predict(forest)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest builds multiple decision trees where each decision tree (each 'estimator') is trained on a different \"re-samplings\" of the training data. Specifically, decision tree $j$ is trained on a new training set $\\mathcal{D}_j$ that is built by sampling $N$ pairs $(\\mathbf{x}_i, y_i)$ from the original $N$ training examples in $\\mathcal{D}$. The sampling is done \"with replacement,\" meaning that the new training set $\\mathcal{D}_j$ may be contain duplicates and/or be missing some of the original data.\n",
    "\n",
    "With the *random_state* you have chosen, the three re-samplings of $\\mathcal{D}=\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), (\\mathbf{x}_3, y_3) \\}$ are:\n",
    "* $\\mathcal{D}_1 = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_1, y_1), (\\mathbf{x}_3, y_3) \\}$ (red, red, green).\n",
    "* $\\mathcal{D}_2 = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), (\\mathbf{x}_2, y_2) \\}$ (red, red, blue).\n",
    "* $\\mathcal{D}_3 = \\{(\\mathbf{x}_2, y_2), (\\mathbf{x}_2, y_2), (\\mathbf{x}_3, y_3) \\}$ (blue, blue, green).\n",
    "\n",
    "Each individual decision tree is not a good classifier.\n",
    "To make a prediction, the random forest collects a prediction from each tree, and returns the class with the most \"votes.\" The winner of these votes tends to be a good classification.\n",
    "\n",
    "**Plot the decision region and decision tree** for each of the three decision trees that comprise the random forest. Read about the *estimators_* attribute in the [*RandomForestClassifier* documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Your answer should use a for-loop *over the estimators*, generating a plot for each one. Your code should generate three figures, where each figure contains two subplots (use Matplotlib's [subplot](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.subplot.html) function). Your first pair should look like this:\n",
    "\n",
    "![image](img/random-forest-simple-estimator0.png)\n",
    "\n",
    "*Hint:* Remember to use `plt.figure()` to start a new figure. Pass argument `figsize=(8,4)` to the *figure* function to make a figure that's twice as wide as it is tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your plotting loop here. Aim for 8-10 lines.\n",
    "for i, tree in enumerate(forest.estimators_):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_predict(tree)\n",
    "    plot_data(X, y)\n",
    "    plt.title(\"Decision regions for estimator %d\" % i)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sklearn.tree.plot_tree(tree, feature_names=['x1', 'x2']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how \"votes\" from the above decision trees are combined by a random forest, you should .\n",
    "\n",
    "**Plot the class probabilities** reported by the *RandomForestClassifier* instance that you trained. Use Matplotlib's *figure*, *subplot*, and *colorbar* functions to create a single figure with three rows (one per class). For each subplot, use the *plot_class_probability* function (see preamble) to plot the heatmap of probablities. (This function calls *RandomForestClassifier*'s **[predict_proba](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba)** method.) Your plot should look like this, but with the probabilities filled in:\n",
    "\n",
    "![image](img/random-forest-simple-class-probabilities.png)\n",
    "\n",
    "It's important not to confuse the number of trees with the number of classes, which in this case are both three.\n",
    "\n",
    "*Tip:* As the figure you need to generate has three subplots in a row, I suggest setting *figsize=(13,3)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 7-8 lines.\n",
    "plt.figure(figsize=(13,3))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plot_class_probability(forest, i)\n",
    "    plt.colorbar()\n",
    "    #plt.imshow(np.ones((1,1,3)), extent=(-3, 3, -3, 3))\n",
    "    plot_data(X, y)\n",
    "    #plt.text(-1.9, 1.2, '(fill with probabilities)')\n",
    "    plt.title(\"Probability of being class %d\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand how the decision tree predictions lead to these probabilities. Inspect the shape of the decision regions proposed by each individual decision tree, and then ensure that you understand their correspondence with the class probabilities above.\n",
    "\n",
    "Finally, once you understand how the three decision trees are combined, **re-run all code cells of Exercise 1.3** but use *n_estimators=10* so that there are now 10 decision trees. Notice how the final class probabilities change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.4 &mdash; Train and inspect a small random forest on a synthetic pattern\n",
    "\n",
    "You are asked to train a random forest classifier on a synthetic binary classification data set. Use the **[sklearn.datasets.make_circles](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html)** function to create two concentric circles.\n",
    "\n",
    "**Write a few lines of code** to:\n",
    "1. Generate the circle data, with each circle made from 10 points. Use *random_state*=0.\n",
    "2. For each *n_estimators* in $\\{1, 2, 4, 8, 16, 32, 64\\}$, train a random forest (use *random_state*=0) and plot its decision regions.\n",
    "\n",
    "You first plot should look like this:\n",
    "![image](img/random-forest-circle-decision-regions.png)\n",
    "\n",
    "Take note of the correspondence between \"more trees\" and the quality of the combined decision regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 8-10 lines.\n",
    "X, y = sklearn.datasets.make_circles(20, random_state=0)\n",
    "\n",
    "for i in range(7):\n",
    "    forest = sklearn.ensemble.RandomForestClassifier(random_state=0, n_estimators=2**i)\n",
    "    forest.fit(X, y)\n",
    "\n",
    "    plt.figure()\n",
    "    plot_predict(forest)\n",
    "    plot_data(X, y)\n",
    "    plt.title(\"Random forest with %d tree(s)\" % len(forest.estimators_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the class probabilities** of the final random forest (the one with *n_estimators*=64) using the *plot_class_probability* function. You should generate one figure with two subplots, each with a colour bar, similar to Exercise 1.3. Your figure should look like this:\n",
    "\n",
    "![image](img/random-forest-circle-class-probabilities.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 7-9 lines.\n",
    "plt.figure(figsize=(11, 4))\n",
    "for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plot_class_probability(forest, i)\n",
    "    plt.colorbar()\n",
    "    #plt.imshow(np.ones((1,1,3)), extent=(-3, 3, -3, 3))\n",
    "    plot_data(X, y)\n",
    "    #plt.text(-1.5, 2.0, '(fill with probabilities)')\n",
    "    plt.title(\"Probability of class %d\" % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how even though the decision region is compacy, the class probabilities have axis-aligned structure extending far away from the data.\n",
    "\n",
    "Finally, **scale the second feature by a factor of two and re-run all code cells of Exercise 1.4**. You can scale the second feature by multiplying the second column in you $X$ matrix by $2$. You should see your circle vertically stretched.\n",
    "\n",
    "After seeing all the results, make note of whether the decision regions simply scaled with the data or whether the decision regions qualitatively changed in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 2. Fitting a Random Forest classifier to real data\n",
    "\n",
    "Exercises 2.1&ndash;2.5 ask you to train and evaluate decision tree and random forest classifiers across multiple hyperparameters on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.1 &mdash; Load data from a CSV file and plot it\n",
    "\n",
    "In this exercise you'll be loading the **[data_train.csv](data_train.csv)** and **[data_test.csv](data_test.csv)** files accompanying this lab. Here's a preview of the training data file:\n",
    "```\n",
    "sepal_length,sepal_width,label\n",
    "4.9,2.4,1\n",
    "4.8,3.0,0\n",
    "5.1,3.3,0\n",
    "7.7,3.0,2\n",
    "6.2,2.8,2\n",
    "...\n",
    "```\n",
    "This is part of the classic [Iris flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
    "The first two comman-separated columns are features. They encode characteristics of flowers. The last column is the class label, where each integer represents one of *Iris Setosa* (0), *Iris Versicolour* (1), and *Iris Virginica*.\n",
    " \n",
    "**Write a few lines of code** to load each CSV file from disk. From the first file you should create variables *X_train* and *y_train* to refer to the training features (float64) and training labels (int32) respectively. From the second file you should likewise create variables *X_test* and *y_test*. Use the **[np.loadtxt](https://numpy.org/devdocs/reference/generated/numpy.loadtxt.html)** function to load the CSV like you did in Lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 6-8 lines.\n",
    "data_train = np.loadtxt('data_train.csv', delimiter=',', skiprows=1)\n",
    "X_train = data_train[:,:-1]\n",
    "y_train = data_train[:,-1].astype(np.int32)\n",
    "\n",
    "data_test = np.loadtxt('data_test.csv', delimiter=',', skiprows=1)\n",
    "X_test = data_test[:,:-1]\n",
    "y_test = data_test[:,-1].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'X_train' in globals(), \"No X_train variable!\" \n",
    "assert 'y_train' in globals(), \"No y_train variable!\" \n",
    "assert 'X_test' in globals(), \"No X_test variable!\" \n",
    "assert 'y_test' in globals(), \"No y_test variable!\" \n",
    "assert isinstance(X_train, np.ndarray)\n",
    "assert isinstance(y_train, np.ndarray)\n",
    "assert isinstance(X_test, np.ndarray)\n",
    "assert isinstance(y_test, np.ndarray)\n",
    "assert X_train.shape == (80,2), \"X_train was wrong shape!\"\n",
    "assert X_train.dtype in (np.float32, np.float64), \"X_train was wrong data type!\"\n",
    "assert y_train.shape == (80,), \"y_train was wrong shape!\"\n",
    "assert y_train.dtype == np.int32, \"y_train was wrong data type!\"\n",
    "assert X_test.shape == (70,2), \"X_test was wrong shape!\"\n",
    "assert X_test.dtype in (np.float32, np.float64), \"X_test was wrong data type!\"\n",
    "assert y_test.shape == (70,), \"y_test was wrong shape!\"\n",
    "assert y_test.dtype == np.int32, \"y_test was wrong data type!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the training and testing data** by completing the plotting code below, much like you did in Exercise 2.1 of Lab 4. When you run the code cell it will generate two figures, and the first one should look like this:\n",
    "\n",
    "![image](img/iris-data-train.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_extent = (3.8, 8.3, 1.6, 5)   # A good (x1min, x1max, x2min, x2max) for plotting iris data\n",
    "\n",
    "def plot_iris_data(X, y, title):\n",
    "    # Your plotting code here. Aim for 3 lines, plus a few for labels/title/legend/limits.\n",
    "    plt.scatter(X[y==0,0], X[y==0,1], s=30, c='r', marker='x', label='Setosa')\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], s=30, c='b', marker='x', label='Versicolour')\n",
    "    plt.scatter(X[y==2,0], X[y==2,1], s=30, c='g', marker='x', label='Virginica')\n",
    "    plt.xlabel('sepal length')\n",
    "    plt.ylabel('sepal width')\n",
    "    plt.title(title)\n",
    "    plt.xlim(iris_extent[0:2])\n",
    "    plt.ylim(iris_extent[2:4])\n",
    "    plt.legend();\n",
    "    \n",
    "plt.figure()\n",
    "plot_iris_data(X_train, y_train, 'Iris training data')\n",
    "\n",
    "plt.figure()\n",
    "plot_iris_data(X_test, y_test, 'Iris test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.2 &mdash; Train a decision tree on the Iris data and plot it\n",
    "\n",
    "You must train a decision tree on the Iris training data, and plot the decision regions. Your final result should look like this:\n",
    "\n",
    "![image](img/iris-decision-tree-decision-region.png)\n",
    "\n",
    "**Write a few lines of code** to:\n",
    "1. Train a *DecisionTreeClassifier* object on the training data.\n",
    "2. Plot the resulting predictor. Use *random_state*=0. \n",
    "3. Add text to your plot showing the accuracy of the classifier when predicting on the training data and when predicting on the held-out test data. Use the **[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)** function and Matplotlib's **[text](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.text.html)** function.\n",
    "\n",
    "For plotting, you cannot use the *plot_predict* function from Exercise 1 because it assumes extents (-3, 3, -3, 3), but you can adapt that code to work for the range of Iris data below.\n",
    "\n",
    "*Tip:* When you are formatting a string like `\"%.1f\" % accuracy` and you want to add a `%` symbol to the string, use two `%%` symbols in a row as in `\"%.1f%%\" % accuracy`. This lets Python know not to expect a second value to substitute into the string, and to just print `%` in that spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iris_predict(model):\n",
    "    # Your prediction plotting code here. Aim for 8-12 lines.\n",
    "    x1min, x1max ,x2min, x2max = iris_extent\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1min, x1max, 100), np.linspace(x2min, x2max, 100))\n",
    "    X = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    y = model.predict(X).reshape(x1.shape)\n",
    "    cmap = matplotlib.colors.ListedColormap(['r', 'b', 'g'])\n",
    "    plt.imshow(y, extent=iris_extent, origin='lower', alpha=0.4, vmin=0, vmax=2, cmap=cmap)\n",
    "    plt.xlim([x1min, x1max])\n",
    "    plt.ylim([x2min, x2max])\n",
    "    plt.gca().set_aspect('equal')\n",
    "    \n",
    "# Your training, plotting, and evaluation code here. Aim for 7-9 lines.\n",
    "tree = sklearn.tree.DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "    \n",
    "plot_iris_predict(tree)\n",
    "plot_iris_data(X_train, y_train, 'Decision tree regions')\n",
    "\n",
    "accuracy_train = sklearn.metrics.accuracy_score(y_train, tree.predict(X_train))\n",
    "accuracy_test  = sklearn.metrics.accuracy_score(y_test, tree.predict(X_test))\n",
    "plt.text(4, 4.7, \"train accuracy: %.1f%%\" % (100*accuracy_train));\n",
    "plt.text(4, 4.5, \" test accuracy: %.1f%%\" % (100*accuracy_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the decision tree** using the *plot_tree* function. You'll need to use the *figure* function with *figsize*=(16,16) in order to make the figure large enough to see all the details. Remember the hint about ending a line with a semicolon (`;`). \n",
    "\n",
    "*Ask yourself: is this tree reasonably \"interpretable\" in your view?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2 lines.\n",
    "plt.figure(figsize=(16,16))\n",
    "sklearn.tree.plot_tree(tree);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.3 &mdash; Train decision trees of different depths\n",
    "\n",
    "You are asked to train multiple decision trees, each with a different *max_depth* parameter, and plot the resulting decision regions and training/test accuracy. The maximum depth of a decision tree controls how finely the tree is allowed to split the feature space before it must predict a class label. A depth of 1 means it is only allowed one split. Your first figure should look like this:\n",
    "\n",
    "![image](img/iris-decision-tree-max-depth-1.png)\n",
    "\n",
    "**Write a few lines of code** to create nine figures: one for each value of *max_depth* $\\in \\{1, \\ldots, 9\\}$. Use a for-loop in your answer. On each iteration, train a new classifier using *random_state*=0. Use the *plot_iris_data* and your *plot_iris_predict* functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 9-12 lines.\n",
    "for depth in range(1, 10):\n",
    "    tree = sklearn.tree.DecisionTreeClassifier(random_state=0, max_depth=depth)\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    plt.figure()\n",
    "    plot_iris_predict(tree)\n",
    "    plot_iris_data(X_train, y_train, 'Decision tree regions (max_depth=%d)' % depth)\n",
    "    \n",
    "    accuracy_train = sklearn.metrics.accuracy_score(y_train, tree.predict(X_train))\n",
    "    accuracy_test  = sklearn.metrics.accuracy_score(y_test, tree.predict(X_test))\n",
    "    plt.text(4, 4.7, \"train accuracy: %.1f%%\" % (100*accuracy_train))\n",
    "    plt.text(4, 4.5, \" test accuracy: %.1f%%\" % (100*accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the training accuracy as *max_depth* increases? What is the maximum test accuracy of a decision tree classifier on this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.4 &mdash; Train random forests of different depths\n",
    "\n",
    "**Repeat Exercise 2.3** but this time train a *RandomForestClassifier* instead of a *DecisionTreeClassifier*. Use *random_state*=0 and *n_estimators*=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 9-12 lines.\n",
    "for depth in range(1, 9):\n",
    "    forest = sklearn.ensemble.RandomForestClassifier(random_state=0, max_depth=depth, n_estimators=100)\n",
    "    forest.fit(X_train, y_train)\n",
    "\n",
    "    plt.figure()\n",
    "    plot_iris_predict(forest)\n",
    "    plot_iris_data(X_train, y_train, 'Random forest regions (max_depth=%d)' % depth)\n",
    "    \n",
    "    accuracy_train = sklearn.metrics.accuracy_score(y_train, forest.predict(X_train))\n",
    "    accuracy_test  = sklearn.metrics.accuracy_score(y_test, forest.predict(X_test))\n",
    "    plt.text(4, 4.7, \"train accuracy: %.1f%%\" % (100*accuracy_train))\n",
    "    plt.text(4, 4.5, \" test accuracy: %.1f%%\" % (100*accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the training accuracy as *max_depth* increases? What is the maximum test accuracy of a random forest classifier on this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.5 &mdash; Evaluate training and test accuracy across two hyperparameters\n",
    "\n",
    "The performance of *RandomForestClassifier* depends on many hyperparameters. Here you'll perform a sweep over *max_depth* (as you did in Exercise 2.4) and also *n_estimators* (as you did in Exercise 1.4).\n",
    "\n",
    "You must evaluate the training and test accuracy of *RandomForestClassifier* for every combination of\n",
    "* *max_depth* $\\in \\{1, 2, 3, 4, 5, 6, 7, 8, 9\\}$\n",
    "* *n_estimators* $ \\in \\{1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024\\}$\n",
    "\n",
    "The results should be compiled into a two-dimensional ndarray and plotted as a heatmap with properly labeled axes. Your training accuracy plot should look like this:\n",
    "![image](img/iris-random-forest-training-accuracy.png)\n",
    "\n",
    "**Write code** to evaluate the training and testing accuracy for all the above combinations. Since training many models may take a few seconds, the plotting should be done in a subsequent code cell, without having to re-run the experiment.\n",
    "\n",
    "*Tip:* If you build a 2-dimensional array of accuracies and plot them using Matplotlib's *imshow* function, Matplotlib does not know which values of *max_depth* each row $0,\\ldots,8$ corresponds to, nor does it know which value of *n_estimators* each column $0,\\ldots,10$ corresponds to. You can specify the values to use via the **[xticks](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.xticks.html)** function, where for example `plt.xticks([0,1,2], [10,20,40])` would cause Matplotlib to display labels *10, 20, 40* at the $x$-axis positions where it would normally display *0, 1, 2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your training and evaluation code here. Aim for 8-12 lines.\n",
    "max_depth_grid = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "n_estimators_grid = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "accuracies = np.empty((len(max_depth_grid), len(n_estimators_grid), 2))\n",
    "\n",
    "for i, max_depth in enumerate(max_depth_grid):\n",
    "    for j, n_estimators in enumerate(n_estimators_grid):\n",
    "        forest = sklearn.ensemble.RandomForestClassifier(random_state=0, max_depth=max_depth, n_estimators=n_estimators)\n",
    "        forest.fit(X_train, y_train)\n",
    "        accuracies[i,j,0] = sklearn.metrics.accuracy_score(y_train, forest.predict(X_train))\n",
    "        accuracies[i,j,1] = sklearn.metrics.accuracy_score(y_test, forest.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your plotting code here. Aim for 8-10 lines.\n",
    "for i, name in enumerate(['training', 'test']):\n",
    "    plt.figure()\n",
    "    plt.imshow(accuracies[:,:,i], origin='lower')\n",
    "    plt.colorbar();\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('max_depth')\n",
    "    plt.xticks(range(len(n_estimators_grid)), labels=n_estimators_grid)\n",
    "    plt.yticks(range(len(max_depth_grid)), labels=max_depth_grid)\n",
    "    plt.title(\"Accuracy on %s data\" % name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what combination(s) of (*max_depth*, *n_estimators*) does the random forest classifier have highest accuracy on the test data? Is the accuracy trend on the test data different than the trend on the training data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
