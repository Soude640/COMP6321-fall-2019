{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 - Neural Networks\n",
    "\n",
    "In this lab you'll use *neural networks* to classify images using both [scikit-learn](https://scikit-learn.org) and [PyTorch](https://pytorch.org/). Both of these libraries are already installed on the lab machines. You will also see that logistic regression is a special case of neural networks.\n",
    "\n",
    "**Run the code cell below** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.preprocessing     # For StandardScaler\n",
    "import sklearn.linear_model      # For LogisticRegression\n",
    "import sklearn.neural_network    # For MLPClassifier\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)                          # Annoying\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)  # Annoying\n",
    "np.set_printoptions(precision=3, suppress=True)  # Print array values as 0.0023 instead of 2.352e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 1. Digit classification with neural networks in scikit-learn\n",
    "\n",
    "Exercise 1.1&ndash;1.8 ask you to load and train a model on the classic MNIST data set. It's so classic it has its [own Wikipedia page](https://en.wikipedia.org/wiki/MNIST_database)! The MNIST data set contains 60,000 training examples and 10,000 test examples. Each example comprises a 784-dimensional feature vector $\\mathbf{x}_i$ representing 28x28 grayscale image of a hand-written digit (784 = 28x28) with a label $y_i \\in \\{0, \\ldots, 9\\}$.\n",
    "\n",
    "Since there are 60,000 training cases, the matrix of training features $\\mathbf{X}$ is provided in as a 60000x784 matrix of pixel intensities. Value $X_{i,j} \\in \\{0, \\ldots, 255\\}$ represents the intensity (0=black, 255=white) of pixel number $j$ in training image $i$. Each 784-dimensional feature vector $\\mathbf{x}_i$ can be reshaped into a 28x28 image as depicted below.\n",
    "\n",
    "![image](img/mnist_data_layout.png)\n",
    "\n",
    "**Run the code cell below** to define a function that will be useful for plotting matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix_grid(V):\n",
    "    \"\"\"\n",
    "    Given an array V containing stacked matrices, plots them in a grid layout.\n",
    "    V should have shape (K,M,N) where V[k] is a matrix of shape (M,N).\n",
    "    \"\"\"\n",
    "    assert V.ndim == 3, \"Expected V to have 3 dimensions, not %d\" % V.ndim\n",
    "    k, m, n = V.shape\n",
    "    ncol = 8                                     # At most 8 columns\n",
    "    nrow = min(4, (k + ncol - 1) // ncol)        # At most 4 rows\n",
    "    V = V[:nrow*ncol]                            # Focus on just the matrices we'll actually plot\n",
    "    figsize = (2*ncol, max(1, 2*nrow*(m/n)))     # Guess a good figure shape based on ncol, nrow\n",
    "    fig, axes = plt.subplots(nrow, ncol, sharex=True, sharey=True, figsize=figsize)\n",
    "    vmin, vmax = np.percentile(V, [0.1, 99.9])   # Show the main range of values, between 0.1%-99.9%\n",
    "    for v, ax in zip(V, axes.flat):\n",
    "        img = ax.matshow(v, vmin=vmin, vmax=vmax, cmap=plt.get_cmap('gray'))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    fig.colorbar(img, cax=fig.add_axes([0.92, 0.25, 0.01, .5]))   # Add a colorbar on the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 Load MNIST and plot some digits\n",
    "\n",
    "The MNIST training data has been provided to you in a file called `mnist_train.npz`. The file is located in the same directory as this Jupyter Notebook.  A `npz` file is an efficient way to store multiple Numpy arrays in a file. Use Numpy's **[load](https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html)** function to open an `npz` file. When the file is opened, you can think of the file as being a Python dictionary where you can ask for an array by its name (its 'key'). The example below shows how to open the file and list the keys:\n",
    "\n",
    "```python\n",
    ">>> with np.load(\"mnist_train.npz\") as data:\n",
    "...    print(list(data.keys()))\n",
    "\n",
    "['X', 'y']\n",
    "```\n",
    "(The reason we open the file using a *with*-statement is because once the *with*-statement is complete the file (\"file descriptor\") is automatically closed, rather than Python trying to keep the file open. This isn't important for the lab, closing files when you're done with them is good programming practice.)\n",
    "\n",
    "\n",
    "**Write a few lines of code** to load the training data from `mnist_train.npz` and create two global vaiables *X_train* and *y_train* to refer to the data you loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 3 lines.\n",
    "with np.load(\"mnist_train.npz\") as data:\n",
    "    X_train = data['X']\n",
    "    y_train = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect the data** by printing information about the arrays.\n",
    "1. Print the shape and dtype of both your *X_train* and *y_train* arrays.\n",
    "2. Print the first five training samples from *X_train* and *y_train* arrays.\n",
    "\n",
    "Since your *X_train* array is big, and because most of the first/last pixels in each image are 0 (black), to see any patterns in the features try printing a slice of values taken from the \"middle\" of each image. For example, pixels 400:415 are roughly from the middle row of each image (similar to blue rectangle in the diagram earlier), so try printing a slice of just those pixels. You should see `[  0   0   0   0   0  81 240 253 253 119  25   0   0   0   0]` printed for the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code for printing shape and dtype here. Aim for 2 lines.\n",
    "print(\"X:\", X_train.shape, X_train.dtype)\n",
    "print(\"y:\", y_train.shape, y_train.dtype)\n",
    "\n",
    "# Your code for printing sample values. Aim for 2 lines.\n",
    "print(X_train[0:5,400:415])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot a few digits** to see what they look like. Use the *plot_matrix_grid* function defined earlier. To do this, you'll need to reshape the array referred to by your *X_train* variable so that the plotting code knows the images have shape 28x28 rather than being just 784-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "plot_matrix_grid(X_train.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the patterns you printed when inspecting the *X_train* variable earlier, and make sure you see where they come from in the first five images plotted above.\n",
    "\n",
    "If you want to see more of the MNIST training digits, rather than just the first few, you can try plotting different \"slices\" of the *X_train* variable, such as *X_train[100:]* to start plotting at the 101st training example. (You still have to reshape the resulting array, of course.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **load the MNIST test data** from the file `mnist_test.npz`, just like you did for the training data. Create global variables *X_test* and *y_test* to refer to the arrays that you loaded. These arrays will be used to evaluate test-time accuracy later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 3 lines.\n",
    "with np.load(\"mnist_test.npz\") as data:\n",
    "    X_test = data['X']\n",
    "    y_test = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 1.2 Preprocess the MNIST data\n",
    "\n",
    "Certain models trained on MNIST work better when the features are normalized. Use scikit-learn to normalize the MNIST data using scaling, such as the *StandardScaler*. (You can just treat the pixels as independent features, nothing fancy.)\n",
    "\n",
    "**Write a few lines of code** to normalize both you *X_train* and *X_test* variables. You can just over-write those variables with the new (normalized) feature arrays, and discard the original unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 3-4 lines.\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the rescaled training digits** using the *plot_matrix_grid* function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "plot_matrix_grid(X_train.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pixels in the center tend to be scaled down more than the pixels in the periphery. *Do you understand why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 1.3 Train multinomial logistic regression on MNIST\n",
    "\n",
    "Train a **[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** object to classify MNIST digits. The *LogisticRegression* class handles multi-class targets, but by default it will use the \"one-vs-rest\" (OvR) strategy.\n",
    "\n",
    "* Use the *multi_class* argument to specify that you want just a single multinomial logistic regression model to be trained, rather than 10 binary logistic regression models.\n",
    "* In order to use multinomial logistic regression, you will also need to specify the `lbfgs` *solver*.\n",
    "* Use *random_state*=0 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-3 lines.\n",
    "lr = sklearn.linear_model.LogisticRegression(multi_class='multinomial',\n",
    "                                             solver='lbfgs', random_state=0)\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the **[score](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score)** method of the *LogisticRegression* object to compute the accuracy as a number in the range $[0.0, 1.0]$. You can convert this accuracy rate into an error rate by taking $(1.0 - \\textrm{accuracy})$. You can then turn it into a percentage by multiplying by 100.\n",
    "\n",
    "**Print the training _error rate_ and testing _error rate_** of your logistic regression model on the MNIST data set. Your output should be in the form:\n",
    "```\n",
    "X.XX% training error\n",
    "X.XX% testing error\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-4 lines.\n",
    "print(\"%.2f%% training error\" % (100*(1-lr.score(X_train, y_train))))\n",
    "print(\"%.2f%% testing error\"  % (100*(1-lr.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the testing error rate you see compare to some of the error rates mentioned on the [MNIST Wikipedia page](https://en.wikipedia.org/wiki/MNIST_database)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the predicted class probabities** of the **first five examples** in the training set. Use the *predict_proba* method of your *LogisticRegression* object. The first row of output should look something like\n",
    "```\n",
    "[0.001 0.    0.    0.203 0.    0.796 0.    0.    0.    0.   ]\n",
    "```\n",
    "From the above probabilities we can see that the model thinks the first digit in the training set is *probably* digit \"5\" but *might also be* digit \"3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "print(lr.predict_proba(X_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 1.4 Visualize the weights of your logistic regression model\n",
    "\n",
    "The logisitic regression model you trained in Exercise 1.3 has a *coef_* attribute. This attribute is the array of weights $\\mathbf{W}$ seen in Lecture 4 (e.g. slide 28). For the MNIST data, this matrix has shape (10, 784), because there are 10 output classes and 784=28x28 pixels. Weight $W_{k,j}$ is the weight with which of pixel $j$ contributes to output class $k$.\n",
    "\n",
    "You are asked to visualize the weights using *plot_matrix_grid*. You may need to reshape the weight matrix to do this. The first two outputs, corresponding to predictin digit \"0\" and predicting digit \"1\" should look something like this:\n",
    "\n",
    "![image](img/mnist_logistic_regression_weights.png)\n",
    "\n",
    "Notice how the pattern for \"0\" is has strong negative weights in the center: that's because if there are white pixels in the center, it's unlikely that the image represents digit \"0\"!\n",
    "\n",
    "**Write a few lines of code** to plot the weights and see what patterns they contain. You should see ten patterns. (Don't worry if the last few grid entries are just white boxes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "plot_matrix_grid(lr.coef_.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an input image (of a hand-written digit) causes one of these patterns to have a large positive response (strong activation), then the corresponing class $\\{0, 1, 2, \\ldots, 9\\}$ will be given a high probability by the final softmax operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 1.5 Train a neural network on MNIST with *zero* hidden layers\n",
    "\n",
    "Train a neural network on MNIST using the **[sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)** class.\n",
    "\n",
    "A neural network has *many* more hyperparameters to configure. Configure your neural network as follows:\n",
    "* Ask for *no hidden layers*. You can do this by specifying an empty tuple `()` for the *hidden_layer_sizes* argument. This will create a neural network where the 784 input features are directly 'connected' to the 10 output predictions, which in this case corresponds to the multinomial logistic regression you did in Exercise 1.4.\n",
    "* Use the `sgd` solver. This means *stochastic gradient descent* that we saw in Lecture 1.\n",
    "* Use a batch size of 100. This means that at each step of SGD the gradient will be computed from only 100 of the 60,000 training cases. This is also callsed a \"mini-batch\". The SGD algorithm works by starting with the firs 100, then the next 100, and then it gets to the last 100 in the training set it starts from the beginning again.\n",
    "* Use *max_iter*=10. This causes the training to stop after SGD has passed over all 60,000 training cases exactly 10 times.\n",
    "* Use *learning_rate_init*=0.01, which determines the step size for SGD once it has computed a gradient.\n",
    "* Use *momentum*=0.9, which speeds up training.\n",
    "* Use *alpha*=0.05 so that the weights are regularized, i.e. are encouraged to \"decay\" to 0.0. This is also called the weight penalty.\n",
    "* Use *random_state*=0 for reproducibility\n",
    "* Use *verbose*=True to see progress printed out. Each time it prints \"Iteration X\" it means SGD has made another pass over all 60,000 training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines, plus whatever line wrapping you need for arguments!\n",
    "mlp = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=True, random_state=0)\n",
    "\n",
    "mlp.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the training error rate and test error rate** of your neural network classifier, just like you did for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-4 lines.\n",
    "print(\"%.2f%% training error\" % (100*(1-mlp.score(X_train, y_train))))\n",
    "print(\"%.2f%% testing error\"  % (100*(1-mlp.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 1.6 Visualize the weights of a neural network (no hidden layers)\n",
    "\n",
    "The *MLPClassifier* object has a *coefs_* attribute that works just like the *coef_* attribute that contained coefficient matrix $\\mathbf{W}$ of *LogisticRegression*, except that for a neural network there are two differences:\n",
    "1. *coefs_* is a *list* of coefficient matrices, so *coefs_[0]* is $\\mathbf{W}^{(1)}$, the coefficient matrix of the *first layer*. Since the neural network you trained in Exercise 1.5 has no hidden layers, this\n",
    "$\\mathbf{W}^{(1)}$ matrix holds the same weights as the $\\mathbf{W}$ matrix for LogisticRegression.\n",
    "2. The weight matrix for *MLPClassifier* has a different layout: it is 784x10 rather than 10x784. Do you now how to account for this?\n",
    "\n",
    "**Write a few lines of code** to repeat Exercise 1.4 but this time with the neural network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "plot_matrix_grid(mlp.coefs_[0].T.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your patterns look streaky then you may need to try transposing your weight matrix to account for the different layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 1.7 Train and visualize the weights of a neural network with 1 hidden layer\n",
    "\n",
    "Here you're asked to train a neural network like you did in Exercise 1.5, but this time **add a hidden layer with 16 'tanh' hidden units** to your neural network. Then you'll visualize the weights of this network.\n",
    "\n",
    "Read the documentation for **[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)** to learn how to do specify a hidden layer. (*Note:* In Python if you want to create a *tuple* object with only one item in it, you can use *(item,)* with an extra comma, rather than *(item)*, which Python interprets to just be regular parentheses.) All the other hyperparameters can stay the same as Exercise 1.5.\n",
    "\n",
    "**Write a few lines of code** to train a new neural network, this time with 16 *tanh* hidden units. In other words, this will be a 784-16-10 neural network where the hidden layer uses *tanh* activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines, plus whatever line wrapping you need for arguments!\n",
    "mlp_h16 = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(16,), activation='tanh',\n",
    "                                               solver='sgd', batch_size=100, max_iter=10,\n",
    "                                               learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                               verbose=True, random_state=0)\n",
    "\n",
    "mlp_h16.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the training error rate and test error rate** of your neural network classifier, just like you did for logistic regression. How does your error rate compare to multinomial logistic regression? (Exercises 1.3 and 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-4 lines.\n",
    "print(\"%.2f%% training error\" % (100*(1-mlp_h16.score(X_train, y_train))))\n",
    "print(\"%.2f%% testing error\"  % (100*(1-mlp_h16.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the first-layer weights $\\mathbf{W}^{(1)}$ of your neural network** using the *plot_matrix_grid* function, just in Exercise 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "plot_matrix_grid(mlp_h16.coefs_[0].T.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are now 16 patterns, not 10, and they no longer seem to correspond to the digits $\\{0,1,\\ldots,9\\}$ in any particular order. *Do you understand why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the second-layer weights $\\mathbf{W}^{(2)}$ of your neural network** using the *plot_matrix_grid* function. \n",
    "\n",
    "However, this time if you inspect the shape of the second weight matrix, *coefs_[1]*, you'll see that it has shape $(16, 10)$, and so it cannot be reshaped into a 28x28 pattern. In fact the second layer has only dimension: the \"hidden layer\" is just a vector of 16 values (the 16 tanh-transformed activations of the first-layer patterns). Each of the 10 output units has 16 weights contributing to it, rather than 784 weights like in Exercise 1.6.\n",
    "\n",
    "Figure out how to reshape the weight matrix so that when you call *plot_matrix_grid* you see a grid of 1x16 weight vectors, like the two examples below:\n",
    "![image](img/mnist_mlp_hidden_weights.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "plot_matrix_grid(mlp_h16.coefs_[1].T.reshape(-1, 1, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 1.8 Train a neural network with lots of hidden units\n",
    "\n",
    "Repeat Exercise 1.7 but with two hidden layers having **100 and 50 hidden units** respectively. This time use **_relu_ activations**. All other hyperparameters can stay the same.\n",
    "\n",
    "**Write a few lines of code** to train the model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines, plus whatever line wrapping you need for arguments!\n",
    "mlp_h100 = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,50), activation='relu',\n",
    "                                                solver='sgd', batch_size=100, max_iter=10,\n",
    "                                                learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                                verbose=True, random_state=0)\n",
    "\n",
    "mlp_h100.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the training and testing error rates** here. *How do they compare to earlier models?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-4 lines.\n",
    "print(\"%.2f%% training error\" % (100*(1-mlp_h100.score(X_train, y_train))))\n",
    "print(\"%.2f%% testing error\"  % (100*(1-mlp_h100.score(X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the first-layer weights $\\mathbf{W}^{(1)}$ of your neural network** here. *Are the pattern detectors here qualitatively different than for earlier models?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 1-2 lines.\n",
    "plot_matrix_grid(mlp_h100.coefs_[0].T.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't bother plotting the 2nd and 3rd layer weights, they are high-dimensional and hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 2. Neural networks in PyTorch\n",
    "\n",
    "Exercise 2.1&ndash;2.2 ask you to train a simple neural network in **[PyTorch 1.2](https://pytorch.org/docs/1.2.0/)**. Here you'll use PyTorch to train an MNIST classifier using the same MNIST data that you already preprocess in Part 1. The goal is just to get you familiar with PyTorch basics and how they compare to scikit-learn.\n",
    "\n",
    "PyTorch is a deep learning framework like TensorFlow. PyTorch tends to be popular with deep learning researchers because it's very flexible for trying new ideas. TensorFlow is also flexible but is designed in such a way that it's more popular for companies trying to deploy high-performance models (in the cloud etc). Both can be used for research, of course! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 2.1 Convert MNIST from Numpy arrays to PyTorch tensors\n",
    "\n",
    "PyTorch has its own Numpy-like array class, called *Tensor*. In order to train a PyTorch model, you must first convert the Numpy arrays. PyTorch understands Numpy arrays, so this is easy. The only tricky part is that, in order to be fast and not waste memory, PyTorch tends to be more picky about the *dtype* of the arrays you give it.\n",
    "\n",
    "**Write a few lines of code** to create two global variables *X_train_torch* and *y_train_torch* that are PyTorch versions of your preprocessed MNIST training data from Part 1. The *X_train_torch* tensor should have *dtype* float32, and the *y_train_torch* tensor should have *dtype* int64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-4 lines.\n",
    "X_train_torch = torch.tensor(X_train.astype(np.float32))\n",
    "y_train_torch = torch.tensor(y_train.astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the code cell below** to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'X_train_torch' in globals(), \"You didn't declare an X_train_torch variable!\"\n",
    "assert 'y_train_torch' in globals(), \"You didn't declare a y_train_torch variable!\"\n",
    "assert isinstance(X_train_torch, torch.Tensor)\n",
    "assert isinstance(y_train_torch, torch.Tensor)\n",
    "assert X_train_torch.dtype == torch.float32\n",
    "assert y_train_torch.dtype == torch.int64\n",
    "assert X_train_torch.shape == (60000,784)\n",
    "assert y_train_torch.shape == (60000,)\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 2.2 Train a PyTorch neural network *without* hidden layers\n",
    "\n",
    "This exercise asks you to run existing code that defines a simple logistic model, and then you are asked to modify the code to add hidden layers to the network.\n",
    "\n",
    "Useful documentation for understanding the code that you see:\n",
    "* **[torch.nn](https://pytorch.org/docs/1.2.0/nn.html)** (neural network)\n",
    "* **[torch.optim](https://pytorch.org/docs/1.2.0/optim.html)** (optimizers such as SGD)\n",
    "\n",
    "Here are some comments to help you understand the \"starter code\" below:\n",
    "\n",
    "* A neural network is a sequence of non-linear transformations, so PyTorch provides a _[Sequential](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Sequential)_ class that accepts a list of desired transformations.\n",
    "\n",
    "* In a standard neural network, the transformations are just linear, i.e. $\\mathbf{Wx}+\\mathbf{b}$, and in PyTorch this is implemented by a _[Linear](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.Linear)_ class where constructing one of these objects with *Linear(D, M)* tells the new object that it should be expecting an *D*-dimensional input and transform it into a *M*-dimensional output. To do this, the *Linear* object will create its own parameter matrix $\\mathbf{W} \\in \\mathbb{R}^{M\\times D}$ and bias vector $\\mathbf{b} \\in \\mathbb{R}^M$.\n",
    "\n",
    "* In PyTorch, the _[CrossEntropyLoss](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.CrossEntropyLoss)_ class conveniently combines applying a softmax and then computing the negative log likelihood. Once you have a *CrossEntropyLoss* object, you can call it with your predictions and targets (both vectors), and it will compute the negative log likelihood, which is just one number (a scalar).\n",
    " \n",
    "\n",
    "\n",
    "**Run the code cell below** to define a simple 784-10 neural network (i.e. logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "# Create an object that can evaluate a neural network.\n",
    "# YOU WILL MODIFY THIS CODE LATER\n",
    "model = torch.nn.Sequential(\n",
    "    #torch.nn.Linear(28*28, 10),   # When data fed through, does Wx+b from 784 dimensions down to 10\n",
    "    torch.nn.Linear(28*28, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 50),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(50, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the code cell below** to define some objects and variables needed for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object that can compute \"negative log likelihood of a softmax\"\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Use stochastic gradient descent to train the model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Use 100 training samples at a time to compute the gradient.\n",
    "batch_size = 100\n",
    "\n",
    "# Make 10 passes over the training data, each time using batch_size samples to compute gradient\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the code cell below** to train the neural network using stochastic gradient descent (SGD). *Note that if you re-run this code cell multiple times it will \"continue\" training from the current parameters, and if you want to \"reset\" the model you need to re-run the earlier code cell that defined the model!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    # Make an entire pass (an 'epoch') over the training data in batch_size chunks\n",
    "    for i in range(0, len(X_train), batch_size):        \n",
    "        X = X_train_torch[i:i+batch_size]   # Slice out a mini-batch of features\n",
    "        y = y_train_torch[i:i+batch_size]   # Slice out a mini-batch of targets\n",
    "\n",
    "        y_pred = model(X)                   # Make predictions (final-layer activations)\n",
    "        l = loss(y_pred, y)                 # Compute loss with respect to predictions\n",
    "        \n",
    "        model.zero_grad()                   # Reset all gradient accumulators to zero (PyTorch thing)\n",
    "        l.backward()                        # Compute gradient of loss wrt all parameters (backprop!)\n",
    "        optimizer.step()                    # Use the gradients to take a step with SGD.\n",
    "        \n",
    "    print(\"Epoch %d final minibatch had loss %.4f\" % (epoch+1, l.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the code cell below** to retrieve the PyTorch model's parameters, convert them back to Numpy, and plot them like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b, *_ = model.parameters()\n",
    "W = W.detach().numpy()\n",
    "\n",
    "# Your code here. Call plot_matrix_grid and aim for 1-2 lines of code.\n",
    "plot_matrix_grid(W.reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this Exercise 2.2, **edit the code for this exercise** to implement a **784-100-50-10 neural network** with **_relu_ activations** just like you did in Exercise 1.8.\n",
    "\n",
    "\n",
    "To do this, you will need to: add more layers to your model, including the **[ReLU](https://pytorch.org/docs/1.2.0/nn.html#torch.nn.ReLU)** object in PyTorch. If you succeed, you should be able to get the training loss to go to zero, especially if you run the training loop multiple times.\n",
    "\n",
    "We will do more PyTorch in the next lab, with convolutional neural networks and recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
